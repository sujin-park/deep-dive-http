## 9장 웹 로봇
+ 특성
  - 인간과 상호작용 없이 웹 트랜잭션을 자동/연속적으로 수행하는 S/W 프로그램
  - 자동으로 웹 사이트 탐색, 콘텐츠 수집, 데이터 처리 등을 수행
  - 종류에 따라 크롤러, 스파이더, 웜, 봇 등으로 불림
+ 예시

  |명칭|역할|
  |---|---|
  |주식 그래프 로봇| 매 분 주식시장 서버로부터 데이터 수집해 추이 그래프를 생성|
  |웹 통계조사 로봇| 웹을 탐색하며 페이지 수/크기/언어/미디어타입 등 수집 <br> => 웹의 규모와 진화를 기록|
  |검색엔진 로봇| 발견한 모든 문서 수집해 검색 DB 구축|
  |가격비교 로봇| 온라인 쇼핑몰 페이지 수집해 상품에 대한 가격 DB 구축|

### 9.1 크롤러와 크롤링
+ 크롤러
  - 웹 링크를 재귀적으로 따라가는 로봇
  - 동작: 문서 검색 -> 문서 내의 URL 링크들 파싱 -> 파싱된 링크를 크롤링 목록에 추가 -> 다음 문서 검색
+ 스파이더
  - 검색엔진이 검색 DB를 구축하기 위해 문서 수집 시 사용하는 크롤러
  - 수집된 문서는 후처리를 통해 검색 가능한 DB화
+ 루트집합
  - == 크롤링 위한 시드 목록
  - 크롤러가 방문을 시작하는 URL의 초기 집합
  - 모든 링크를 크롤링하면 관심있는 페이지의 대부분이 수집되도록 설정 필요
  - 예시
    * A, G, S 선택
    * ![루트집합 예시 그림](http://tlog.tammolo.com/static/141830e92a932461f96af6f300bed3a8/eac10/Untitled-06436312-7456-4303-8d7f-9bb0a186518e.png)
+ 순환
  - 문제점
    * 루프 (loops): 크롤러를 루프에 빠뜨려 동일 페이지 수집 및 시간/대역폭 낭비 초래 가능 
    * 서비스 방해: 페이지 반복요청 시 서버 부담 증가 및 사용자 접근 저해 가능 => 법적문제 발생 소지
    * 중복 (dups): 중복 페이지 수집 시 애플리케이션에 쓸모없는 콘텐츠 쌓이게 됨
  - => 크롤러가 방문한 URL 기억해 순환 방지 필요
+ 방문기록 관리
  - 수억 개의 URL은 많은 공간을 차지 (예: 40Byte * 1억 = 약 4GB)
  - URL 관리방법

    |명칭|상세|
    |---|---|
    |트리 & 해시 테이블| 검색트리나 해시테이블 자료구조로 URL을 저장|
    |느슨한 존재 비트맵|해시함수로 변환된 존재 비트 배열 (presence bit array)로 관리 <br> => 저장공간 최소화 (but 느슨한 배열로 충돌가능성 존재)|
    |체크포인트|갑작스런 중단 대비해 방문 목록 저장되었는지 확인|
    |파티셔닝|- 대규모 웹로봇에게 URL의 특정 부분 할당해 처리 <br> - 로봇들은 서로 돕거나 소통하며 활동을 조정|

    > 농장(farm): 분리된 컴퓨터로 존재하는 각 로봇이 동시에 일하고 있는 곳

#### 9.1.6 별칭(alias) & 로봇순환
+ URL이 별칭 가질 수 있으므로 서로 다른 URL이 동일 리소스에 할당될 수 있음
+ 예시
  - 기본 포트: `www.test.kr:80`
  - URL인코딩: `www.test.kr/~kim` = `www.test.kr/%7kim`
  - 태그 링크: `www.test.kr/home.html#title`
  - 대소문자 비구별: `www.test.kr/home.html` = `www.test.kr/HOME.html`
  - 기본 index: `www.test.kr` = `www.test.kr/index.html`
  - IP주소: `www.test.kr` = `217.233.88.99`

#### 9.1.7 URL 정규화
+ 많은 로봇이 URL 정규화를 통해 중복 가능성 제거 시도 
+ 방식
  - 포트번호 명시되지 않은 경우 호스트명에 `:80` 추가
  - 모든 ％ 이스케이핑 문자를 대응되는 문자로 변환
  - `#` 태그 제거 
+ but) 대소문자, index, IP주소 문제는 서버 설정 알아야 대응 가능

#### 9.1.8 파일 시스템 링크 순환
+ 파일 시스템의 심벌릭 링크로 인해 순환 발생 가능
+ 대부분 실수로 생성되나 악의적으로 생성되기도
+ 예시
  - 부모 가리키는 심볼릭 링크를 실제 디렉토리로 착각해 루프 발생
  - `www.test.kr/list.html` -> `www.test.kr/updir/list.html` -> `www.test.kr/updir/updir/list.html`

#### 9.1.9 동적 가상 웹 공간
+ 요청 시마다 새로운 가상 URL과 콘텐츠 만드는 동적 공간 존재 시 루프 발생
+ 예시: 다음달 계속 표시하는 달력 콘텐츠

#### 9.1.10 루프와 중복 피하기
+ 모든 순환 피하는 방법 없으므로 순환 회피 위한 휴리스틱 집합 필요
+ 웹 로봇의 동작기법

  |명칭|상세|
  |---|---|
  |URL 정규화|URL을 표준형태로 변환해 중복 리소스로 연결되는 URL 수집을 일부 회피|
  |너비 우선 크롤링|횡적 검색 우선해 순환에 의한 부작용 최소화|
  |스로틀링|웹 사이트에서 시간당 수집하는 페이지 수를 제한 => 접근/중복 횟수 제한|
  |URL 크기 제한|일정 길이(예:1KB) 넘는 URL 제외 <br> => 긴 URL에 의한 서버실패/순환 방지 <br> but) 수집 못하는 페이지 발생 가능|
  |URL/사이트 블랙리스트|순환 등의 문제 발생하는 사이트를 블랙리스트에 추가해 회피|
  |패턴 발견|URL의 반복요소 검사해 심볼릭 링크 등에 의한 순환을 회피|
  |콘텐츠 지문|페이지의 체크썸을 계산해 중복 콘텐츠 회피 <br> but) 동적 콘텐츠/동작 존재 시 감지 어려움|
  |사람의 모니터링|로봇은 결국 어떤 기법으로도 해결 안 되는 문제 봉착할 수 있음 <br> => 상용 수준의 로봇은 사람이 진단/로깅 등으로 모니터링해야|

### 9.2 로봇의 HTTP
+ 요청 헤더
  - 로봇도 클라이언트처럼 HTTP 명세 지켜야
  - 많은 로봇이 요구사항 적은 HTTP/1.0 요청을 보냄
+ 신원식별 헤더
  - 로봇의 능력/신원/출신 알리는 기본적 헤더를 전송해야
    * 잘못된 크롤러 소유자 탐색 시 유용
    * 서버가 로봇의 콘텐츠 처리 능력 확인 가능 
  - 기본적 신원식별 헤더

    |명칭|상세|
    |---|---|
    |User-Agent| 로봇의 이름 제공|
    |From| 로봇의 사용자/관리자 이메일 주소 제공|
    |Accept| 수신 가능한 미디어 타입|
    |Referer| 요청 URL 포함하고 있던 문서의 URL 제공|

+ 가상 호스팅
  - 가상 호스팅이 널리 퍼져있으므로 요청에 Host 헤더 포함해야 정확한 콘텐츠 탐색 가능
  - 예시: 여러 사이트 운영되는 서버에 index 요청하는 경우
+ 조건부 요청
  - 시간/엔터티 태그 비교해 마지막 수집 시점 이후의 문서만 수집 가능
  - => 수집 콘텐츠량 현저히 감소 가능
+ 응답 다루기
  - 상태코드
    * 일반적인 상태코드와 그들의 분류 다룰 수 있어야 (예: 200, 404)
    * 에러 메세지를 상태코드 200으로 처리하는 경우에 유의
  - 엔터티
    * 로봇은 헤더에 임베딩된 정보로 엔터티 자체의 정보 취득 가능
    * 예: http-equib meta 태그 파싱해 헤더에 덮어쓸 정보를 취득
      ```
      <!-- Refresh 헤더: 1초 후 home.html로 리다이렉트하라-->
      <meta http-equiv="Refresh" content="1; URL=home.html">
      ```
+ User-Agent 타겟팅
  - 웹 관리자들은 로봇의 방문과 요청 예상해 대응 전략 세워야
  - 로봇 접근 시 frame 미지원 오류 등 발생하지 않도록 대응 필요  
    => 저기능 브라우저 및 로봇 지원하는 콘텐츠 개발해야

### 9.3 부적절한 로봇 동작
|명칭|상세|
|---|---|
|폭주|사람보다 빠른 요청 속도로 인해 서버에 과부하 유발 가능|
|오래된 URL|없어진 URL 요청 시 에러로그 및 부하 증가|
|길고 무의미한 URL|순환/오류 등으로 긴 URL 요청 시 서버 처리능력/로그에 부정적 영향|
|민감 데이터 수집|비밀번호/카드정보 등 민감 데이터 수집 시 무시/제거해야|
|동적 게이트웨이 접근|게이트웨이 애플리케이션 콘텐츠 요청 시 처리비용 크게 증가 가능|

