## 9장 웹 로봇
+ 특성
  - 인간과 상호작용 없이 웹 트랜잭션을 자동/연속적으로 수행하는 S/W 프로그램
  - 자동으로 웹 사이트 탐색, 콘텐츠 수집, 데이터 처리 등을 수행
  - 종류에 따라 크롤러, 스파이더, 웜, 봇 등으로 불림
+ 예시

  |명칭|역할|
  |---|---|
  |주식 그래프 로봇| 매 분 주식시장 서버로부터 데이터 수집해 추이 그래프를 생성|
  |웹 통계조사 로봇| 웹을 탐색하며 페이지 수/크기/언어/미디어타입 등 수집 <br> => 웹의 규모와 진화를 기록|
  |검색엔진 로봇| 발견한 모든 문서 수집해 검색 DB 구축|
  |가격비교 로봇| 온라인 쇼핑몰 페이지 수집해 상품에 대한 가격 DB 구축|

### 9.1 크롤러와 크롤링
+ 크롤러
  - 웹 링크를 재귀적으로 따라가는 로봇
  - 동작: 문서 검색 -> 문서 내의 URL 링크들 파싱 -> 파싱된 링크를 크롤링 목록에 추가 -> 다음 문서 검색
+ 스파이더
  - 검색엔진이 검색 DB를 구축하기 위해 문서 수집 시 사용하는 크롤러
  - 수집된 문서는 후처리를 통해 검색 가능한 DB화
+ 루트집합
  - == 크롤링 위한 시드 목록
  - 크롤러가 방문을 시작하는 URL의 초기 집합
  - 모든 링크를 크롤링하면 관심있는 페이지의 대부분이 수집되도록 설정 필요
  - 예시
    * A, G, S 선택
    * ![루트집합 예시 그림](http://tlog.tammolo.com/static/141830e92a932461f96af6f300bed3a8/eac10/Untitled-06436312-7456-4303-8d7f-9bb0a186518e.png)
+ 순환
  - 문제점
    * 루프 (loops): 크롤러를 루프에 빠뜨려 동일 페이지 수집 및 시간/대역폭 낭비 초래 가능 
    * 서비스 방해: 페이지 반복요청 시 서버 부담 증가 및 사용자 접근 저해 가능 => 법적문제 발생 소지
    * 중복 (dups): 중복 페이지 수집 시 애플리케이션에 쓸모없는 콘텐츠 쌓이게 됨
  - => 크롤러가 방문한 URL 기억해 순환 방지 필요
+ 방문기록 관리
  - 수억 개의 URL은 많은 공간을 차지 (예: 40Byte * 1억 = 약 4GB)
  - URL 관리방법

    |명칭|상세|
    |---|---|
    |트리 & 해시 테이블| 검색트리나 해시테이블 자료구조로 URL을 저장|
    |느슨한 존재 비트맵|해시함수로 변환된 존재 비트 배열 (presence bit array)로 관리 <br> => 저장공간 최소화 (but 느슨한 배열로 충돌가능성 존재)|
    |체크포인트|갑작스런 중단 대비해 방문 목록 저장되었는지 확인|
    |파티셔닝|- 대규모 웹로봇에게 URL의 특정 부분 할당해 처리 <br> - 로봇들은 서로 돕거나 소통하며 활동을 조정|

    > 농장(farm): 분리된 컴퓨터로 존재하는 각 로봇이 동시에 일하고 있는 곳

#### 9.1.6 별칭(alias) & 로봇순환
+ URL이 별칭 가질 수 있으므로 서로 다른 URL이 동일 리소스에 할당될 수 있음
+ 예시
  - 기본 포트: `www.test.kr:80`
  - URL인코딩: `www.test.kr/~kim` = `www.test.kr/%7kim`
  - 태그 링크: `www.test.kr/home.html#title`
  - 대소문자 비구별: `www.test.kr/home.html` = `www.test.kr/HOME.html`
  - 기본 index: `www.test.kr` = `www.test.kr/index.html`
  - IP주소: `www.test.kr` = `217.233.88.99`

#### 9.1.7 URL 정규화
+ 많은 로봇이 URL 정규화를 통해 중복 가능성 제거 시도 
+ 방식
  - 포트번호 명시되지 않은 경우 호스트명에 `:80` 추가
  - 모든 ％ 이스케이핑 문자를 대응되는 문자로 변환
  - `#` 태그 제거 
+ but) 대소문자, index, IP주소 문제는 서버 설정 알아야 대응 가능

#### 9.1.8 파일 시스템 링크 순환
+ 파일 시스템의 심벌릭 링크로 인해 순환 발생 가능
+ 대부분 실수로 생성되나 악의적으로 생성되기도
+ 예시
  - 부모 가리키는 심볼릭 링크를 실제 디렉토리로 착각해 루프 발생
  - `www.test.kr/list.html` -> `www.test.kr/updir/list.html` -> `www.test.kr/updir/updir/list.html`

#### 9.1.9 동적 가상 웹 공간
+ 요청 시마다 새로운 가상 URL과 콘텐츠 만드는 동적 공간 존재 시 루프 발생
+ 예시: 다음달 계속 표시하는 달력 콘텐츠

#### 9.1.10 루프와 중복 피하기
+ 모든 순환 피하는 방법 없으므로 순환 회피 위한 휴리스틱 집합 필요
+ 웹 로봇의 동작기법

  |명칭|상세|
  |---|---|
  |URL 정규화|URL을 표준형태로 변환해 중복 리소스로 연결되는 URL 수집을 일부 회피|
  |너비 우선 크롤링|횡적 검색 우선해 순환에 의한 부작용 최소화|
  |스로틀링|웹 사이트에서 시간당 수집하는 페이지 수를 제한 => 접근/중복 횟수 제한|
  |URL 크기 제한|일정 길이(예:1KB) 넘는 URL 제외 <br> => 긴 URL에 의한 서버실패/순환 방지 <br> but) 수집 못하는 페이지 발생 가능|
  |URL/사이트 블랙리스트|순환 등의 문제 발생하는 사이트를 블랙리스트에 추가해 회피|
  |패턴 발견|URL의 반복요소 검사해 심볼릭 링크 등에 의한 순환을 회피|
  |콘텐츠 지문|페이지의 체크썸을 계산해 중복 콘텐츠 회피 <br> but) 동적 콘텐츠/동작 존재 시 감지 어려움|
  |사람의 모니터링|로봇은 결국 어떤 기법으로도 해결 안 되는 문제 봉착할 수 있음 <br> => 상용 수준의 로봇은 사람이 진단/로깅 등으로 모니터링해야|

### 9.2 로봇의 HTTP
+ 요청 헤더
  - 로봇도 클라이언트처럼 HTTP 명세 지켜야
  - 많은 로봇이 요구사항 적은 HTTP/1.0 요청을 보냄
+ 신원식별 헤더
  - 로봇의 능력/신원/출신 알리는 기본적 헤더를 전송해야
    * 잘못된 크롤러 소유자 탐색 시 유용
    * 서버가 로봇의 콘텐츠 처리 능력 확인 가능 
  - 기본적 신원식별 헤더

    |명칭|상세|
    |---|---|
    |User-Agent| 로봇의 이름 제공|
    |From| 로봇의 사용자/관리자 이메일 주소 제공|
    |Accept| 수신 가능한 미디어 타입|
    |Referer| 요청 URL 포함하고 있던 문서의 URL 제공|

+ 가상 호스팅
  - 가상 호스팅이 널리 퍼져있으므로 요청에 Host 헤더 포함해야 정확한 콘텐츠 탐색 가능
  - 예시: 여러 사이트 운영되는 서버에 index 요청하는 경우
+ 조건부 요청
  - 시간/엔터티 태그 비교해 마지막 수집 시점 이후의 문서만 수집 가능
  - => 수집 콘텐츠량 현저히 감소 가능
+ 응답 다루기
  - 상태코드
    * 일반적인 상태코드와 그들의 분류 다룰 수 있어야 (예: 200, 404)
    * 에러 메세지를 상태코드 200으로 처리하는 경우에 유의
  - 엔터티
    * 로봇은 헤더에 임베딩된 정보로 엔터티 자체의 정보 취득 가능
    * 예: http-equiv meta 태그 파싱해 헤더에 덮어쓸 정보를 취득
      ```
      <!-- Refresh 헤더: 1초 후 home.html로 리다이렉트하라-->
      <meta http-equiv="Refresh" content="1; URL=home.html">
      ```
+ User-Agent 타겟팅
  - 웹 관리자들은 로봇의 방문과 요청 예상해 대응 전략 세워야
  - 로봇 접근 시 frame 미지원 오류 등 발생하지 않도록 대응 필요  
    => 저기능 브라우저 및 로봇 지원하는 콘텐츠 개발해야

### 9.3 부적절한 로봇 동작
|명칭|상세|
|---|---|
|폭주|사람보다 빠른 요청 속도로 인해 서버에 과부하 유발 가능|
|오래된 URL|없어진 URL 요청 시 에러로그 및 부하 증가|
|길고 무의미한 URL|순환/오류 등으로 긴 URL 요청 시 서버 처리능력/로그에 부정적 영향|
|민감 데이터 수집|비밀번호/카드정보 등 민감 데이터 수집 시 무시/제거해야|
|동적 게이트웨이 접근|게이트웨이 애플리케이션 콘텐츠 요청 시 처리비용 크게 증가 가능|

### 9.4 로봇 차단하기
+ robots.txt
  - 1994년 제안된 로봇의 동작/접근 제어하는 단순하고 자발적인 기법
  - 정식명칭은 Robots Exclusion Standard, 그냥 `robots.txt`로도 불린다
  - 로봇에게 접근 권한 안내하는 문서로 주로 서버의 문서 루트에 생성함
  - 파일 시스템이 아닌 게이트웨이나 애플리케이션에서 동적으로 생성하기도
  - 표준 따르는 로봇은 크롤링 전에 `robots.txt`를 먼저 확인함 

#### 9.4.1 로봇 차단 표준
+ 특성
  - 임시방편으로 마련된 표준이며 로봇의 접근제어 능력이 불완전하나
  - 주류 업체 대부분과 검색엔진 크롤러가 표준을 지원
+ 버전
  - 대부분의 로봇이 v0.0이나 v1.0을 지원
  - v2.0은 복잡해 채택률 낮음 
   
  |버전|날짜|상세|
  |---|---|---|
  |0.0|1994.06|- Martjin Koster의 로봇 배제 표준 <br> - Disallow 지시자 지원하는 오리지널 robots.tx 메커니즘|
  |1.0|1996.11|- Martjin Koster의 웹 로봇 제어방법 <br> - Allow 지시자 지원 추가된 IETF 초안|
  |2.0|1996.11|- Sean Conner의 로봇차단을 위한 확장 표준 <br> - 정규식/타이밍 정보가 포함되나 널리 지원되지는 않음 <br> - [An Extended Standard for Robot Exclusion](http://www.conman.org/people/spc/robots2.html)|

+ 참조
  - [로봇 배제 표준](https://ko.wikipedia.org/wiki/%EB%A1%9C%EB%B4%87_%EB%B0%B0%EC%A0%9C_%ED%91%9C%EC%A4%80) 
  - [Robots Exclusion Protocol](http://www.robotstxt.org/robotstxt.html) 
  - [robots.txt Maker/Generator](https://w3seo.info/robots-txt) 
  - [로봇 배제 프로토콜 (REP) 표준 공식화 (2019)](https://webmaster-ko.googleblog.com/2020/04/rep.html)/[Eng](https://developers.google.com/search/blog/2019/07/rep-id)
  - [Robots Exclusion Protocol (2020)](https://tools.ietf.org/html/draft-rep-wg-topic-00) 

#### 9.4.2 robots.txt 파일
+ 규칙
  - 한 사이트에 `robots.txt` 존재 시 로봇은 특정 URL 방문 전 반드시 해당 파일 확인해야
  - 호스트명 + 포트 조합에 대한 `robots.txt` 파일은 하나만 존재
  - 가상 호스팅 이용 중이라면 각 가상 docroot 마다 `robots.txt` 파일 존재 가능
  - 웹 마스터는 사이트의 모든 콘텐츠에 대한 차단 규칙이 종합적으로 기술된 `robots.txt` 파일 생성할 책임 가짐
+ robots.txt 요청
  - HTTP GET 메소드로 요청
    * `robots.txt` 파일 존재 시 text/plain 본문으로 반환됨
    * `404(Not Found)` 응답 시 로봇 접근 제한하지 않는 것으로 인식
  - 식별헤더를 통해 신원정보 및 연락처 제공해야
  - ex)
    ```
    GET /robots.txt HTTP/1.0
    Host: www.test.kr
    User-Agent: Slurp/2.0
    Date: Wed Dec 17 18:22:35 EST 1969
    ```

+ 응답코드에 따른 동작

  |응답코드|동작|
  |---|---|
  |2XX (OK)|응답 콘텐츠 파싱해 차단규칙 취득하고 따름|
  |404 (Not Found)|차단규칙 없는 것으로 판단|
  |401 (Unauthorized) <br> 403 (Forbidden))|사이트 접근 완전히 제한된다고 판단해야|
  |503 (Service Unvailable)|리소스 검색을 뒤로 미뤄야|
  |3XX (Redirect)|리소스 발견 시까지 리다이렉트 따라가야|

#### 9.4.3 robots.txt 파일 형식
+ 구성
  - 빈 줄, 주석 줄, 규칙 줄로 구성
  - 규칙 줄
    * HTTP 헤더처럼 기술 (필드:값)
    * 패턴 매칭에 사용 (대소문자 구별X)
    * 레코드로 구분되며 순서대로 적용
  - 레코드
    * 특정 로봇들 집합에 대한 차단 규칙의 집합을 기술 => 로봇별로 규칙 적용 가능
    * User-Agent: 해당되는 로봇 기술
    * Allow/Disallow: 해당되는 URL 기술
  - 주석 줄
    * 파일 내 어디에나 기입 가능
    * `#`로 시작해 `줄바꿈` 문자로 종료 
+ ex) 
  ```
  # slurp과 webcrawler에게 secure 이외 부분 크롤링 허용
  # 나머지 로봇들은 사이트의 어디에도 접근 불가
  
  User-Agent: slurp
  User-Agent: webcrawler
  Disallow: /secure
  
  User-Agent: *
  Disallow:
  ```

+ 접두매칭(prefix matching)
  - Allow/Disasllow 규칙은 규칙 경로의 길이만큼 일치해야 하며 대소문자도 동일해야 
  - % 이스케이핑된 문자는 원래 값으로 복원후 비교되며 대소문자 구별 X
  - 빈 문자열은 모든 값에 매칭
  - `v1.0`은 부모 디렉토리와 무관하게 특정 디렉토리명에 대한 규칙 지정 불가

#### 9.4.4 robots.txt 파싱규칙
+ `robots.txt` 파일 명세 발전해 추가 필드 포함될 수 있음
+ 로봇은 이해 못하는 필드 존재 시 무시해야
+ `v0.0`은 Allow 줄 미지원 => 일부 로봇은 Allow줄 무시할 것
+ 하위 호환성 때문에 한 줄을 여러 줄로 나누기 허용 안 됨

#### 9.4.5 robots.txt 캐싱 & 만료
+ 로봇은 서버 부하 줄이기 위해 주기적으로 `robots.txt` 파일 가져와 캐시해야
+ 캐시된 `robots.txt` 파일은 만료시까지 사용되며 HTTP 캐시 메커니즘 따라 관리됨
+ 많은 크롤러가 HTTP/1.1 클라이언트 아님에 주의 필요
  * 캐시 지시자 이해 못 할 수도
  * 로봇 명세 초안은 Cache-Control 지시자 존재 시 7일간 캐싱 권고하나 실무적으로 너무 긴 편
  * 일부 대규모 웹 크롤러는 활발하게 활동하는 동안에는 `robots.txt` 매일 새로 가져옴

#### 9.4.6 로봇 차단 라이브러리
+ 공개된 펄(Perl) 라이브러리가 몇 가지 존재
+ WWW:RobustRules
  - CPAN 공개 펄 아카이브의 모듈
  - `WWW:RobustRules` 객체는 URL에 접근 금지 확인할 수 있는 메소드 제공
  - `WWW:RobustRules` 객체로 여러 `robots.txt` 파일 파싱 가능

#### 9.4.7 HTML 로봇제어 메타태그
+ 특성
  - 메타 테그는 서버 관리자가 아닌 페이지 저자도 직접 작성 가능
  - 로봇들은 문서 취득할 수 있으나 로봇 차단 태그 존재 시 해당 문서 무시할 것
  - `robots.txt` 표준과 동일하게 준수가 권장되나 강제는 아님
  - 지시어 중복/충돌하면 안 됨
  - name, content 값은 대소문자 구별 X
  - ex) `<meta name="robots" content=dir-list>`
+ 로봇 meta 태그 지시어

  |명칭|상세|
  |---|---|
  |INDEX| 콘텐츠 인덱싱 가능하다고 로봇에게 안내|
  |NOINDEX| 콘텐츠 인덱싱 불가하다고 로봇에게 안내|
  |FOLLOW| 이 페이지가 링크한 페이지 크롤링 가능하다고 로봇에게 안내|
  |NOFOLLOW|이 페이지가 링크한 페이지 크롤링 불가하다고 로봇에게 안내|
  |ALL| INDEX + FOLLOW|
  |NONE| NOINDEX + NOFOLLOW|
  |NOARCHIVE|이 페이지 캐시 사본 만들지 말라고 로봇에게 안내|

+ 검색엔진 meta 태그

  |name|content|상세|
  |---|---|---|
  |DESCRIPTION|<텍스트>|페이지의 짧은 요약 정의 <br> `<meta name="description" content="뫄뫄의 기술블로그에 오신 것을 환영합니다">`|
  |KEYWORDS|<쉼표 목록>|- 페이지 기술하는 키워드 목록 <br> - 구분문자: 쉼표(,) <br> `<meta name="keywords" content="python,statics,analysis">`|
  |REVISIT-AFTER|<숫자 days>|- 재방문 권장 시점 (페이지 변경 예측되므로)<br> - 널리 지원되지는 않음 <br> `<meta name="revisit-after" content="7 days">`|

### 9.5 로봇 에티켓
+ 특성
  - 1993년 마틴 코스터(Martijn Koster)가 웹 로봇 제작자를 위한 가이드라인 목록 작성
  - 참고: https://www.robotstxt.org/guidelines.html
+ 가이드라인

  |항목|세부항목|상세|
  |---|---|---|
  |1) 신원식별|로봇의 신원|HTTP `User-Agent`에 로봇의 명칭/목적/정책URL 등 기술|
  ||기계의 신원|IP주소 안내해 역방향 DNS로 로봇의 소속 식별 가능하게 해야|
  ||연락처|연락 가능한 이메일 주소 제공해야|
  |2) 동작|긴장|로봇에 문의/항의 들어올 수 있음<br>로봇이 노련해지기 전까지 모니터링 필요|
  ||대비|조직의 대역폭 소비하므로 사용 전에 미리 알려야|
  ||감시와 로그|진행상황/동작상태 등 확인 가능하도록 진단/로깅 기능 갖춰야|
  ||학습과 조정|크롤링 시마다 새롭게 배운 것을 반영해 로봇을 조정/개선해야 <br>(예: 함정 피하기)|
  |3) 자가 제한|URL 필터링|이해할 수 없거나 불필요한 데이터의 URL은 무시해야 <br>(예:zip 파일)|
  ||동적 URL 필터링|로봇은 보통 동적 게이트웨이 콘텐츠 수집할 필요 없음 <br>(예:cgi 포함된 URL)|
  ||Accept 헤더 필터링|`Accept` 헤더 사용해 수신 가능 콘텐츠 서버에 알려야|
  ||robots.txt 준수|방문한 웹 사이트의 `robots.txt` 준수해야|
  ||스스로 억제|시간당 요청 수 및 총 접근횟수 제한필요 <br>=> 사이트의 트래픽 소모 방지|
  |4) 문제들 <br>(루프/중복 등)|응답코드 대응|반드시 모든 상태코드 대응 가능해야 <br> 응답코드에 대해 로깅 및 모니터링 필요|
  ||URL 정규화|URL 정규화하여 중복된 접근 방지 필요|
  ||순환 회피|순환 감지/회피 노력 필요 <br> 운영 과정을 피드백 루프화 하여 크롤러 계속 개선해야|
  ||함정 감시|의도적/악의적 순환은 탐지 어려움 <br> 낯선 URL 가진 사이트에 접근 많은 경우 함정일 가능성 높음|
  ||블랙리스트 관리|이상 사이트는 블랙리스트에 추가해 다시는 방문 말아야 <br> (예: 함정/사이클/깨진사이트/로봇 붙잡아두는 사이트)|
  |5) 확장성|공간/규모|웹의 규모, 로봇의 메모리 소모량 등 미리 계산해야|
  ||대역폭|대역폭 사용량 계산 및 모니터링해 최적화 필요 <br> 나가는 요청이 들어오는 요청보다 훨씬 적을 것|
  ||시간|크롤링 소요시간 추정치와 실제치 비교해 문제 파악 필요|
  ||분할|대규모 크롤링 시 여러 기기 필요 <br> 예: 여러 네트워크 가진 대형 멀티프로세서, 협력하는 작은 컴퓨터들 등|
  |6) 신뢰성|사전 테스트|정식 크롤링 실행 전 작은 규모의 테스트 크롤링 진행해봐야 <br> 성능/메모리 소모량 등 분석해 문제 파악 필요 |
  ||체크포인트|여러 종류 실패 발생할 수 있으므로 체크포인트/재시작 기능 설계해야|
  ||실패에 유연|실패 발생해도 계속 동작하도록 설계해야|
  |7) 소통|준비|로봇에 대한 항의 빠르게 대응 가능하도록 준비해야 <br>정책 안내 페이지  생성하고 `robots.txt` 생성방법 안내해야|
  ||이해|항의자에게 로봇의 중요성보다 로봇 차단규칙을 설명하는 것이 효율적 <br> 항의 지속 시 해당 사이트에서 크롤러 제거하고 블랙리스트에 추가해야|
  ||즉시 대응|항의자에게 즉시/전문적 대응하지 않고 로봇 내버려면 상대가 화낼 것|

### 9.6 검색엔진
+ 특성
  - 웹 로봇을 가장 광범위하게 사용
  - 웹 크롤러들이 검색엔진에 문서를 수집해줘서 문서 내 단어에 대한 색인 생성 가능
  - 수십 억의 웹 페이지에 대해 대규모 크롤러가 병렬로 작업 수행 
+ 검색엔진 아키텍처
  - 전 세계 웹 페이지에 대해 풀 텍스트 색인이라고 하는 로컬 DB 생성
  - 풀 텍스트 색인에 크롤러는 웹 페이지 수집해 추가하고 사용자는 질의를 전송
+ 풀 텍스트 색인
  - full-text indexes
  - 일종의 카탈로그처럼 동작하며 특정 단어 포함된 문서 즉시 알려줄 수 있는 DB
  - 색인 생성 후에는 검색할 필요 없음 (결과 이미 정리된 상태므로)
  - ex) 단어 the -> A문서, B문서 
+ 질의 전송
  - 사용자가 폼 입력 시 HTTP GET이나 POST 요청으로 검색엔진 게이트웨이에 전송
  - 게이트웨이 프로그램은 검색 질의 추출해 풀 텍스트 색인 검색용 표현식으로 변환
+ 검색결과 정렬
  - 게이트웨이 애플리케이션은 사용자 위한 질의 결과 페이지 즉석에서 생성
  - 많은 페이이지가 검색어 내포할 수 있으므로 결과에 순위 매기는 알고리즘 필요
  - 검색어와 관련성 높은 순서대로 문서를 표시하기 위해 결과에 점수 매기고 정렬작업 수행 
    *  = 랭킹(relevancy ranking)
    * 관련 알고리즘/팁/기법은 검색엔진의 기밀
    * 크롤링 시 해당 페이지로 들어오는 링크 수 등 통계 수집해 인기도 측정하기도
+ 스푸핑
  - 속임을 이용한 공격의 총칭
  - 사이트가 검색엔진 상위에 표시되도록 검색 엔진 속이는 기술
  - 예시 
    * 많은 키워드 나열된 가짜 페이지 생성
    * 특정 단어에 대해 가짜 페이지 생성하는 게이트웨이 애플리케이션 사용
