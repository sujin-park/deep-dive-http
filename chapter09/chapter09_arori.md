- 웹 로봇: 사람과의 상호작용 없이 연속된 웹 트랜잭션을 자동으로 수행하는 소프트웨어 프로그램
    - 방식에 따라 '크롤러', '스파이더', '웜', '봇' 등으로 불림
- 웹 로봇의 예시:
    - 주식 시장 서버에 매분 요청을 보내 얻은 데이터를 활용해 그래프를 생성하는 로봇
    - WWW의 규모와 진화에 대한 통계 정보를 수집하기 위해 웹을 떠돌며 페이지의 갯수, 크기, 언어 미디어 타입을 기록하는 로봇
    - 검색 DB를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
    - 상품에 대한 가격 DB를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹페이지를 수집하는 가격 비교 로봇

## 9.1 크롤러와 크롤링

- 웹 크롤러(Web Crawler): 웹 페이지를 한개 가져온 뒤, 그 페이지가 가리키는 모든 웹 페이지를 가져오고, 그 방식을 재귀적으로 반복하는 방식의 웹 순회 로봇.
    - 크롤러 혹은 스파이더라고하며, 하이퍼링크들로 만들어진 웹을 따라 기어다녀서(crawl) 이렇게 명명
- 인터넷 검색엔진은 크롤러를 이용하여 검색 가능한 DB가 됨.

### 9.1.1 어디에서 시작하는가: '루트 집합'

- 크롤러가 방문하는 URL들의 초기 집합 = 루트 집합(root set)
- 모든 문서들 끼리 이어지는 것이 아니므로, 결과적으로 관심 있는 웹 페이지 대부분을 가져올 수 있게 충분히 다른 장소의 여러 URL을 선택해야 함
    - 일반적으로 루트 집합에 너무 많은 페이지가 있을 필요는 없음. (1) 인기 있는 웹 사이트, (2) 새로 생성된 페이지들, (3) 자주 링크되지않는 알려지지 않은 페이지들로 구성 되어있음
    - 대규모 크롤러 제품들은 사용자들이 루트 집합에 새 페이지 혹은 유명하지 않은 페이지를 추가하는 기능 제공

### 9.1.2 링크 추출과 상대 링크 정상화

- 크롤러는 웹을 돌아다니며 꾸준히 HTML 문서를 검색하고, 각 페이지 내의 URL 링크를 파싱해서 크롤링할 페이지 목록에 추가함
- HTML 파싱을 하여 링크를 추출하고, 상대링크를 절대 링크로 변환할 필요가 있음

### 9.1.3 순환 피하기

- 크롤링 중에 멈추거나 진행을 느려지게 하므로 루프나 순환에 빠지지 않도록 해야 함
- 로봇은 순환을 피하기 위해 어디를 방문했는지 알고 있어야 함

### 9.1.4 루프와 중복

- 순환이 크롤러에게 해로운 이유 3가지
    1. **크롤러 무효화**: 루프는 허술하게 만든 크롤러를 계속 빙빙 돌고, 같은 페이지를 가져오는데 모든 시간을 허비하게 만듦. 이런 크롤러가 네트워크 대역폭을 다 차지하고 어떤 페이지도 가져올 수 없게 됨. 
    2. **웹 서버 부담**: 같은 페이지를 반복해서 가져오면 웹 서버에 부담이 됨. 실제 사용자도 사이트에 접근할 수 없도록 될 수도 있으며, 이런 서비스 방해 행위는 법적인 문제제기의 근거가 됨
    3. **중복된 콘텐츠**: 크롤러가 수 많은 중복 페이지를 가져와, 불필요한 중복된 콘텐츠로 넘치게 됨. 예를 들면, 수 백개의 같은 페이지를 반환하는 검색 엔진

### 9.1.5 빵 부스러기의 흔적

- 동적으로 생성된 콘텐츠를 빼도 웹 페이지는 수십 억개 존재해, 방문한 곳을 지속적으로 추적하는 것은 어려움
- 어떤 URL을 방문했는지 빠르게 판단하기 위해서는, 속도와 메모리 면에서 효과적인 복잡한 자료구조 필요
    - 빠른 속도: 많은 URL은 빠른 검색 구조를 요구. 최소 검색 트리나 해시 테이블을 필요로 함
    - 효과적인 메모리: 많은 URL은 많은 공간 차지.
- 대규모 웹 크롤러가 사용하는 유용한 기법
    1. **트리와 해시 테이블(Trees and hash tables)**
        - URL을 을 빨리 찾아볼 수 있게 해주는 자료구조
    2. **느슨한 존재 비트맵(Lossy presence bit maps)**
        - 공간 사용 최소화를 위해, 존재 비트 배열(presence bit array)와 같은 느슨한 자료 구조 사용
        - 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 '존재 비트(presence bit)'을 갖음
        - URL은 크롤링 되었을 때 해당 존재 비트가 만들어 지며, 이미 존재한다면 크롤링 되었다고 간주
    3. **체크 포인트(Checkpoints)**
        - 로봇이 갑작스럽게 중단 될 경우를 대비해 방문한 URL의 목록이 디스크에 저장되었는지 확인
    4. **파티셔닝(Partitioning)**
        - 웹의 성장에 따라, 하나의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것은 불가. 메모리, 디스크 공간, 연산 능력, 네트워크 대역 폭등이 충분치 않음.
        - 대규모 웹 로봇은 각 분리된 한 대의 컴퓨터인 로봇이 동시에 일하는 '농장(farm)'을 이용. 각 로봇은 URL들의 특정한 '한 부분'이 할당되어 그에 대한 책임을 짐
        - 로봇들은 서로 도와 웹을 크롤링. 개별 로봇들은 URL을 넘겨주거나, 오작동하는 동료를 돕거나, 그 외의 활동을 조정하기 위한 커뮤니케이션을 함

### 9.1.6 별칭(alias)과 로봇 순환

- 자료 구조를 잘 갖추어도, URL이 별칭을 가질 수 있어 어떤 페이지를 방문했었는지 알기 어려울 때도 있음.
    - 한 URL이 또 다른 URL 에 대한 별칭이면, 둘이 서로 달라보이더라도 사실은 같은 리소스를 가리킴
- 별칭의 예:
    1. 기본 포트가 80번일 때
        - `http://www.foo.com/bar.html` = `http://www.foo.com:80/bar.html`
    2. URI 인코딩된 것과 안된 것이 같을 때
        - `http://www.foo.com/~fred` = `http://www.foo.com/%7fred`
    3. 태그에 따라 페이지가 바뀌지 않을 때 
        - `http://www.foo.com/x.html#early` = `http://www.foo.com/x.html#middle`
    4. 서버가 대소문자 구분하지 않을 때
        - `http://www.foo.com/readme.htm` = `http:www.foo.com/README.HTM`
    5. 기본 페이지가 index.html 일 때
        - `http://www.foo.com/` = `http://www.foo.com/index.html`
    6. 어떤 URL이 같은 IP주소를 가질 때
        - `http:www.foo.com/index.html` = `http://209.231.87.45/index.html`

### 9.1.7 URL 정규화하기

- 대부분의 웹 로봇은 URL들을 표준 형식으로 '정규화' 함으로써 다른 URL과 같은 리소스를 가리키고 있음이 확실한 것을 미리 제거하려 시도
- 다음과 같은 방식으로 모든 URL을 정규화된 형식으로 변환.
    1. 포트가 명시되지 않았다면, ':80' 추가 (별칭 예 1번 해결)
    2. 모든 %xx 이스케이핑된 문제는 대응되는 문자로 변환 (별칭 예 2번 해결)
    3. `#` 태그 제거 (별칭 예 3번 해결)
- 별칭 4~6번을 해결하는 좋은 방법은 없음
    - 4번 문제 해결을 피하려면 웹 서버가 대소문자를 구분하는지 알 필요가 있음
    - 5번 문제를 해결하려면 디렉터리에 대한 웹 서버의 색인 페이지 설정을 알 필요가 있음
    - 6번은 URL 호스트명과 URL의 IP 주소가 같은 물리적 컴퓨터를 참조하는 것 뿐 아니라, 웹 서버가 가상 호스팅을 하도록 설정되어 있는지도 알아야 함
- URL 정규화는 기본적인 문법의 별칭을 제거할 수 있지만, 로봇들은 URL을 표준 형식으로 변환하는 것만으로는 제거할 수 없는 다른 URL 별칭도 만나게 될 것임

### 9.1.8 파일 시스템 링크 순환

- 파일의 심볼링 링크(symbolic link)는 사실상 아무것도 존재하지 않으면서 끝없이 깊어지는 디렉터리 계층을 만들어, 매우 교묘한 종류의 순환을 유발
    - 심볼링 링크 순환(symbolic link loop)은 보통 서버 관리자가 실수로 만들지만, 때때로 사악한 웹 마스터가 로봇을 함정에 빠트리기위해 악의적으로 만들기도 함
- root에 `subdir` 이라는 심볼릭 링크가 있고 이는 위쪽을 가리킨다면
    1. `GET http://www.foo.com/index.html`
    `subdir/index.html` 로 이어지는 링크 발견
    2. `GET http://www.foo.com/subdir/index.html`
    `subdir/index.html` 로 이어지는 링크 발견
    3. `GET http://www.foo.com/subdir/subdir/index.html`
    `subdir/index.html` 로 이어지는 링크 발견
    4. 이를 계속 반복

### 9.1.9 동적 가상 웹 공간

- 악의적인 웹 마스터들이 의도적으로 복잡한 크롤러 루프를 만들 수 있음
- 평범한 파일처럼 보이지만 사실은 게이트웨이 애플리케이션인 URL을 쉽게 만들 수 있음
    - 이 애플리케이션은 같은 서버에 있는 가상 URL에 대한 링크를 포함한 HTML 파일을 즉석으로 만듦
    - 악의적인 서버는 저 가상의 URL로 요청을 받으면 새 가상 URL을 갖고 있는 새 HTML을 날조하여 만듦
    - URL과 HTML이 매번 전혀 달라 보일 수 있어 로봇이 순환을 감지하기 어려울 수 있음.
- 악의 없이 심볼릭 링크나 동적 콘텐츠를 통해 크롤러 함정이 만들어질 수 있음
    - 한달치 달력을 생성하고 그 다음달로 링크를 걸어주는 CGI 기반의 달력 프로그램이 있다면, 사용자라면 영원히 다음 달 링크를 누르고 있지 않겠지만, 로봇이라면 요청할 수도 있음

### 9.1.10. 루프와 중복 피하기

- 모든 순환을 피하는 완벽한 방법은 없지만, 실제로 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 함
- 로봇 구현자가 생각해야할 트레이드오프(trade off)는 이들 휴리스틱은 문제를 피하는데 도움이 되지만, 동시에 약간의 손실을 유발할수도 있다는 점.
    - 의심스러워 보이지만 실제로 유효한 컨텐츠를 거를 수도 있기 때문
- 웹 상에서 로봇이 더 올바르게 작동하기 위해 사용하는 기법들에 대해 설명

***URL 정규화(Canonicalizing URLs)***

- URL을 표준 형태로 변환해, 같은 리소스를 가리키는 중복된 URL 생기는 것을 일부 회피

***너비 우선 크롤링(Breadth-first crawling)***

- 방문할 URL들을 웹 사이트 전체에 걸쳐 너비 우선으로 스케줄링하면 순환 영향 최소화 가능
- 혹시 로봇 함정을 건드리더라도, 그 순환에서 페이지를 받아오기 전에 다른 웹사이트들에서 페이지를 많이 받아올 수 있음
- 만약 깊이 우선 방식이라면, 순환을 건드리는 경우 영원히 다른 사이트로 나올 수 없음.
- 또한 너비 우선 크롤링은 요청을 더 분산 시켜 특정 서버가 압박 받지 않게 해주는 장점이 있음.
    - 한 로봇이 한 서버의 자원을 최소로 사용하도록 유지

***스로틀링(Throttling)***

- 로봇이 일정 시간동안 가져올 수 있는 페이지의 숫자 제한.
- 순환을 건드려 지속적으로 접근을 시도한다면, 스로틀링을 이용해 서버에 대한 접근, 중복의 총 횟수를 제한 할 수 있음

***URL 크기 제한***

- 일정 길이(보통 1KB)를 넘는 URL의 크롤링은 거부 가능
- 만약 순환으로 인해 URL이 계속 길어진다면, 길이 제한으로 인해 순환이 중단됨
- 하지만 이 기법으로 인해 가져오지 못하는 콘텐츠도 있음.
    - 오늘날 사이트들은 URL을 통해 사용자 상태를 관리하여, URL이 길어질 수 있음
- 이 기법은 요청 URL이 특정 크기가 될 때마다 에러 로그를 남겨, 특정 사이트에서 발생하는 일을 감시하는 사용자에게는 훌륭한 신호가 됨

***URL/사이트 블랙리스트***

- 순환을 만들거나 함정인 것으로 알려진 사이트와 URL 목록을 만들어 관리하여 피함
- 사람의 손을 필요로 함
- 오늘날의 실제 사용중인 대규모 크롤러들은 몇가지 형태의 블랙리스트를 가지고 있음
- 블랙리스트는 크롤링을 원치 않는 사이트를 피하기 위해서도 사용될 수 있음

***패턴 발견(Pattern detection)***

- 파일 시스템의 심볼릭 링크를 통한 순환과 그 비슷한 오설정들은 일정한 패턴이 있음
- 몇 로봇은 반복되는 구성요소를 가진 URL을 잠재적 순환으로 보고, 둘 혹은 셋 이상의 반복된 구성요소를 갖는 URL 크롤링을 거절.
- 단순히 한 디렉터리의 반복(예: `/subdir/subdir/...`)뿐 아니라 주기 길이가 2이상인 반복(예: `/subdir/images/subdir/images/...` 도 감지

***콘텐츠지문(Content fingerprinting)***

- 중복을 감지하는 보다 직접적인 방법
- 콘텐츠 지문을 사용하는 로봇은 페이지의 콘텐츠에서 몇 바이트를 얻어내 체크섬(checksum)을 계산
    - 이 체크섬은 그 페이지 내용의 간략한 표현을 의미
    - 만약 이전에 본 체크섬을 가진 페이지를 가져온다면, 그 페이지의 링크는 크롤링 하지 않음.
- 체크섬 함수는 두 페이지의 내용이 다름에도 체크섬이 같을 확률이 낮은 것을 사용해야 함.
    - 지문 생성용으로는 MD5와 같은 메시지 요약 함수가 인기
- 어떤 웹 서버는 동적으로 페이지를 수정하므로, 로봇은 웹 페이지 콘텐츠에 임베딩된 링크와 같은 특정 부분들을 체크섬 계산에 빠뜨림.
- 페이지 콘텐츠를 임의로 커스터마이징을 포함한 서버의 동적 동작은 중복 감지 방해

***사람의 모니터링***

- 로봇이 어떤 기법으로도 해결할 수 없는 문제에 봉착하면 사람이 확인할 수 있도록, 진단과 로깅을 포함하도록 설계해야 함.

## 9.2 로봇의 HTTP

- 로봇은 다른 클라이언트 프로그램과 다르지 않으며, 똑같이 HTTP 명세의 규칙을 지켜야 함.
- 많은 로봇들이 콘텐츠를 요청하기위해 필요한 HTTP를 최소한으로 구현하려고 함

### 9.2.1 요청 헤더 식별하기

- 최소한으로 HTTP를 지원하려하지만, 약간의 신원 식별헤더(특히 `User-Agent` 헤더)는 구현 및 전송
- 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 가지 헤더를 사이트에게 전송하는 것이 좋음
    - (1) 문제가 있는 크롤러의 소유자를 찾아 낼 때, (2) 로봇이 서버에게 어떤 종류의 콘텐츠를 다룰 수 있는지 정보를 줄 때 유용.
- 구현하도록 권장되는 기본 신원 식별 헤더
    - `User-Agent` : 서버에게 요청을 만든 로봇의 이름을 말해줌
    - `Form` : 로봇의 사용자/관리자의 이메일 주소 제공(RFC 822 이메일 주소 포맷)
    - `Accept` : 서버에게 어떤 미디어 타입을 보내도 되는지 안내.
    - `Referer` : 현재 요청 URL을 포함한 문서의 URL 제공. 어떻게 이 페이지를 찾았는지 알고 싶은 사이트 관리자에게 유용

### 9.2.2 가상 호스팅

- 가상 호스팅이 널리 퍼진 현실에서 요청 헤더에 Host 헤더를 포함하지 않으면 잘못된 콘텐츠를 찾게 됨
    - Host 헤더가 없는 크롤러는 A 사이트에 요청을 보냈지만, 서버가 A와 B를 가상 호스팅하므로 B의 콘텐츠를 돌려 줄 수 있음. 크롤러는 B의 내용을 A로부터 온 것으로 생각하게 됨

### 9.2.3 조건부 요청

- 로봇은 때때로 매우 많은 양의 요청을 시도하므로, 로봇이 검색하는 콘텐츠의 양을 최소화하는 것은 중요
    - 오직 변경된 콘텐츠만 가져오도록 하는 것은 의미가 있음
- 로봇 중 몇몇은 시간이나 엔터티 태그를 비교하여 마지막 버전 이후의 업데이트를 알아보는 조건부 HTTP 요청을 구현. (캐시와 유사)

### 9.2.4 응답 다루기

- 대다수의 로봇은 주 관심사가 GET 메서드로 콘텐츠를 요청해서 가져오기라 응답 다루기라고 할 만한 일을 하지 거의 하지 않음
- 그러나 몇 특정 기능(조건부 요청 등)을 사용거나 웹 탐색이나 서버와의 상호작용을 더 잘하려는 로봇은 여러 종류의 HTTP 응답을 다룰 줄 알아야 함

***상태코드***

- 일반적으로 로봇은 최소한의 일반적인 상태 코드나 예상할 수 있는 상태코드를 다룰 수 있어야 함
    - 200, 404는 이해해야하 하며, 이해할 수없는 상태 코드는 그 코드가 속한 분류에 근거하여 다루어야 함
- 모든 서버가 항상 적절한 에러코드를 반환하지 않는 것에 유의해야 함

***엔터티***

- HTTP 헤더에 임베딩 된 정보를 따라 로봇들은 엔터티 자체의 정보를 찾을 수 있음
- `http-equiv` 태그는 콘텐츠 저자가 콘텐츠를 다루는 서버가 제공할 수도 있는 헤더를 덮어쓰기 위한 수단
- 로봇 구현자는 `http-equiv` 정보를 찾기 위해 HTML문서의 HEAD태그를 탐색하는 것을 원할 수도 있음.
    - 예: `<meta http-equiv="Refresh" content="1; URL=index.html">`
        - Refresh는 리다이렉트용으로 사용되며, 위는 1초후에 index.html로 리다이렉트하라는 의미
    - 명세상으로 `http-equiv` 는 HEAD 섹션에 있어야하지만 이상한 데 있을 수도 있음

### 9.2.5. User-Agent 타겟팅

- 웹 관리자들은 많은 로봇들로 부터의 요청을 예상하고 이를 다루기위한 전략을 세워야 함.
- 많은 웹 사이트들은 브라우저 종류를 감지하여 콘텐츠를 최적화하는데, 이는 로봇에게 콘텐츠 대신 에러페이지를 제공할 수도 있음.
    - 예: 당신의 브라우저는 프레임를 지원하지 않습니다

## 9.3 부적절하게 동작하는 로봇들

***폭주하는 로봇***

- 로봇은 빠르게 HTTP 요청을 만드므로, 논리적 에러를 갖고있거나 순환에 빠진다면 웹 서버에 극심한 부하를 안겨줌
- 모든 로봇 저자는 폭주 방지를 위한 보호 장치를 신경써서 설계해야 함

***오래된 URL***

- URL 목록이 오래되었을 수 있음 = 로봇이 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있음
- 이러한 요청으로 에러 로그가 채워지거나, 웹 서버는 에러페이지를 제공하는 부하 발생

***길고 잘못된 URL***

- 순환이나 프로그래밍 상의 오류로 인해 로봇은 웹 사이트에게 크고 의미 없는 URL 요청 가능
- URL이 많이 길다면 웹 서버 처리능력에 영향을 주고, 접근 로그를 어지럽게하고, 허술한 웹서버라면 고장을 일으킬수도 있음

***호기심이 지나친 로봇***

- 사적인 데이터에 대한 URL을 얻어 그 데이터를 인터넷 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근하도록 만들 수도 있음.

***동적 게이트웨이 접근***

- 로봇들이 항상 접근하고 있는 것에 대해 잘 알고 있는 것은 아님
- 로봇은 게이트웨이 애플리케이션 콘텐츠에 대한 URL을 요청할 수 있고, 이는 보통 특수 목적을 위한 데이터이며 처리비용이 많이 들어 웹 사이트 관리자가 좋아하지 않음

## 9.4 로봇 차단하기

- **로봇 차단/배제 표준(Robots Exclusion Standard)** a.k.a `robots.txt`: 로봇의 접근을 막거나, 웹 마스터가 로봇의 동작을 더 잘 제어할 수 있는 메커니즘을 제공하는 기법. 1994년 제안됨
    - 웹 서버는 서버의 문서 루트에 robots.txt라는 파일을 선택적으로 제공할 수 있고, 이 파일은 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 정보를 담고 있음
    - 로봇이 이 표준을 따른다면 웹 사이트의 다른 리소스를 접근하기 전에 `robots.txt` 를 요청

### 9.4.1 로봇 차단 표준

- 로봇 차단 표준은 임시방편으로 마련된 표준이며, 이 표준이 작성되고 있을 떄, 업체들은 이 표준의 부분 집합을 제각각 구현하고 있었음.
- 사실상 표준(de facto standard)이 되어, 대부분의 주류 업체들과 검색엔진 크롤러들은 이 차단 표준 지원
- 2019년 7 월 1 일 Google에서 IEFT 산하 공식 표준으로 "[Robots Exclusion Protocol](https://tools.ietf.org/html/draft-rep-wg-topic-00)" 을 제안. 아직 초안(draft)이며 승인 절차를 거치는 중.

### 9.4.2 웹 사이트와 robots.txt

- 웹 사이트의 어떤 URL을 방문하기 전, 그 웹 사이트에 `robots.txt` 파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리해야 함.
    - `robots.txt` 파일이라고 부르지만, 꼭 파일시스템에 존재해야할 이유는 없으며 게이트웨이 애플리케이션이 동적으로 생성할 수도 있음
- 한 호스트명과 포트번호에 의해 정의된 어떤 웹 사이트는, 그 사이트 전체에 대한 `robots.txt` 파일을 하나만 가지고 있음
    - 이 책을 쓰고 있는 시점에서 '로컬' `robots.txt` 파일을 웹 사이트의 개별 서브디렉터리에 설치할 수 있는 방법은 없음
- 웹 사이트가 가상 호스팅된다면, 다른 모든 파일과 동일하게 가상의 `docroot` 에 서로 다른 `robots.txt` 가 있을 수 있음

***robots.txt 가져오기***

- 로봇은 어느 파일들과 마찬가지로 HTTP GET 메서드를 이용하여 `robots.txt` 리소스를 가져옴.
    - 존재한다면, 서버는 그 파일을 `text/plain` 본문으로 반환.
    - 존재하지 않는다면, 서버는 `404 Not Found` 로 응답.
        - 로봇은 접근을 제한하지 않는것으로 간주하고 어떤 파일이든 요청.
- 로봇은 사이트 관리자가 로봇의 접근을 추적할 수 있도록 `From` 이나 `User-Agent` 헤더를 통해 신원 정보와 문의나 불만사항을 보낼 연락처를 제공

    ```
    GET /robots.txt HTT/1.0
    Host: www.joes-hardware.com
    User-Agent: Slurp/2.0
    ```

***응답코드***

- 로봇은 `robots.txt` 검색결과에 따라 다르게 동작
    - 상태코드 2XX: 성공으로 응답하면 그 응답의 콘텐츠를 파싱하여 차단 규칙을 얻고, 그 사이트에서 무언가를 가져오려 할 때 그 규칙에 따름
    - 상태코드 404: 리소스가 존재하지 않는다고 응답하면 로봇은 활성화된 차단 규칙이 없다고 가정하고 제약없이 그 사이트에 접근
    - 상태코드 401, 403: 서버가 접근 제한으로 응답하면 로봇은 그 사이트로의 접근은 완전히 제한되어있다고 가정
    - 상태코드 503: 만약 요청 시도가 일시적으로 실패했다면 로봇은 그 사이트의 리소스를 검색하는 것을 뒤로 미루어야 함
    - 상태코드 3XX: 만약 서버 응답이 리다이렉션을 의미한다면 로봇은 리소스가 발견될때까지 리다이렉트를 따라가야 함

### 9.4.3 robots.txt 파일 포맷

- 매우 단순한 줄 기반 문법을 갖으며, 각 줄은 빈줄, 주석줄, 규칙 줄의 세가지 종류가 있음
    - 규칙 줄은 HTTP 헤더처럼 생겼고(`<필드>:<값>`) 패턴 매칭을 위해 사용.

```
# 이 파일은 Slurp, webcrawler에게 공개된 영역을 크롤링하는 것을 허용하고
# 그 외 다른 로봇은 안된다 (주석 영역)

User-Agent: slurp
User-Agent: webcrawler
Disalow: /private

User-Agent: *

```

- 이 줄들은 레코드로 구분되며, 각 레코드는 특정 로봇의 집합에 대한 차단 규칙의 집합을 기술.
    - 이 방법을 통해 로봇 별로 각각 다른 차단 규칙 적용
- 각 레코드는 규칙 줄들의 집합으로 되어 있으며, 빈줄이나 파일 끝(end-of-file) 문자로 끝남.
- 레코드는 어떤 로봇이 레코드에 영향을 받는지 지정하는 하나 이상의 `User-Agent` 줄로 시작하며 뒤이어 이 로봇들이 접근할 수 있는 URL을 말해주는 `Allow` 줄과 `Disallow` 줄이 옴

***User-Agent 줄***

- 각 로봇의 레코드는 하나 이상의 `User-Agent` 줄로 시작함
    - `User-Agent: <robot-name>` 나 `User-Agent: *` 형식을 갖을 수 있음
    - 로봇의 이름은 로봇의 HTTP GET 요청 안의 User-Agent 헤더를 통해 보내짐
- `robots.txt` 를 처리한 로봇은 다음의 레코드에 반드시 따라야 함
    - 로봇 이름이 자신 이름의 부분 문자열이 될 수 있는 레코드 중 첫 번째 것
    - 로봇 이름이 '*'인 레코들 중 첫 번째 것
- 만약 대응하는 `User-Agent` 를 찾지 못하였고, 와일드카드를 사용한 줄도 찾지 못하였다면 대응하는 레코드가 없는 것이므로 접근에 제한이 없음
- 로봇 이름은 대소문자를 구분하지 않는 부분 문자열과 맞춰보므로, 의도치 않게 맞는 경우에 주의해야 함

***Disallow와 Allow 줄들***

- 로봇 차단 레코드의 `User-Agent` 줄 바로 다음에 오며, 특정 로봇에 대한 어떤 URL 경로가 명시적으로 금지/허용 되는지 기술함
- 로봇은 반드시 요청하려고하는 URL을 차단 레코드의 모든 `Disallow` 와 `Allow` 규칙에 순서대로 맞춰보아야 함. 첫 번째로 맞은 것이 사용되며, 아무것도 맞지 않다면 허용된 것으로 처리함
    - robots.txt URL은 항상 허용되어야하고, `Allow` / `Disallow` 규칙에 나타나서는 안됨
- URL과 맞는 하나의 `Allow` / `Disallow` 줄에 대해, 규칙 경로는 반드시 맞춰보고자하는 경로의 대소문자를 구분하는 접두어야 함

***Disallow/Allow 접두 매칭(prefix matching)***

- `Disallow` 나 `Allow` 규칙이 어떤 경로에 적용되려면, 그 경로의 시작부터 규칙경로 길이만큼의 문자열이 규칙 경로와 같아야 함(대소문자 구분)
    - `User-Agent` 와 달리 * 대신 빈 문자열을 이용해 모든 문자열에 매치되도록 할 수 있음
    - 규칙 경로나 URL 경로의 임의의 '이스케이핑된' 문자들(%XX)은 비교 전에 원래대로 복원됨. 빗금(/)을 의미하는 %2F는 예외로 반드시 그대로 매치되어야 함
    - 어떤 규칙 경로가 빈 문자열이라면 그 규칙은 모든 URL 경로와 매치됨
- 규칙 경로 → URL 매치 예시
    - `/tmp` → `/tmp` : 매칭O. 규칙경로 == URL 경로
    - `/tmp` → `/tmpfile.html` : 매칭O. 규칙경로가 URL 경로의 접두어
    - `/tmp` → `/tmp/a.html` : 매칭O. 규칙경로가 URL 경로의 접두어
    - `/tmp/` → `/tmp` : 매칭X. 규칙경로가 URL 경로의 접두어가 아님
    - `/~fred/hi.html %` → `%7Efred/hi.html` : 매칭O. %7E = ~
    - `/%7Efred/hi.html` → `/~fred/hi.html` : 매칭O. %7E = ~
    - `/%7efred/hi.html` → `/%7Efred/hi.html` : 매칭O. 이스케이핑된 문자는 대소문자 비구분
    - `/~fred/hi.html` → `~fred%2Fhi.html` : 매칭X. 빗금은 특별히 정확히 매치되어야 함
- 접두 매칭은 꽤 잘 동작하지만, 어떤 경로 밑에있냐와 상관없이 특정 이름에 대해서 크롤링을 막는 것은 어려움.

### 9.4.4 그 외에 알아둘 점

- `robots.txt` 파일은 명세가 발전함에 따라 `User-Agent`, `Disallow`, `Allow` 외의 다른 필드를 포함할 수 있음. 로봇은 이해하지 못하는 필드는 무시해야 함
- 위 호환성을 위해, 한 줄을 여러 줄로 나누어 적는것은 비허용
- 주석은 파일 어디에서든 사용 가능.
- 로봇 차단 표준의 구버전은 `Allow` 줄을 지원하지 않았음. 몇 오래된 로봇은 `Allow` 줄을 무시함. 이 경우 허용되는 URL도 탐색하지 않음

### 9.4.5 robots.txt의 캐싱과 만료

- 매 파일접근마다 `robots.txt` 파일을 새로가져와야하면 로봇도 비효율적이며 웹 서버 부하도 커짐
- 로봇은 주기적으로 `robots.txt` 파일을 가져와서 결과를 캐시함
    - 캐시된 사본은 만료될 때까지 로봇에 의해 사용 됨
    - `robots.txt` 캐싱 제어를 위해 표준 HTTP 캐시 제어 메커니즘이 원 서버와 로봇 양쪽에서 사용됨
- 크롤러 제품들이 HTTP 버전이 다른 클라이언트일 수 있어, 캐시 지시자를 이해 못할 수도 있는 점에 유의
- 로봇 명세 초안에는 `Cache-Control` 지시자가 있는 경우 7일간 캐싱 명시
    - 실무에서 보면 보통 너무 김.
- 몇 대규모 크롤러는 활발히 크롤링하는 기간에는 매일 새로 가져옴

### 9.4.6 로봇 차단 펄 코드

- `robots.txt` 파일과 상호작용하는 공개된 펄(Pearl) 라이브러리가 몇 있음
- 책에서는 CPAN 공개 펄 아카이브의 WWW::RobustRules 모듈을 사용하며, 자세한 내용은 책 참조

### 9.4.7. HTML 로봇 제어 META 태그

- `robots.txt` 의 단점 중 하나는 그 파일을 콘텐츠 작성자 개개인이아닌 웹 사이트 관리자가 소유한다는 것
- HTML 페이지 저자는 로봇이 개별 페이지에 접근하는 것을 제어하도록 HTML 문서에 로봇 제어 태그를 추가할 수 있음.
- 로봇 제어 HTML 태그에 따르는 로봇들은 문서를 가져올 수 있지만, 로봇 차단 태그가 있다면 문서를 무시
    - `robots.txt` 와 동일하게 따르는 것을 권장하지만 강제하지 않음
- 다른 모든 META 태그와 동일하게 HEAD 섹션에 나타나야 함.

***로봇 메타 태그 지시자***

- `<meta name="robots" content=directive-list>` 와 같이 구현. (대소문자 미구분)
- 지시자에는 종류가 몇가지 있으며, 앞으로 추가될 가능성이 높음
- 지시자 목록 ([참고](https://developer.mozilla.org/ko/docs/Web/HTML/Element/meta/name#%EB%8B%A4%EB%A5%B8_%EB%A9%94%ED%83%80%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%9D%B4%EB%A6%84))
    - NOINDEX: 크롤러가 페이지를 색인하지 않도록 요청
    - NOFOLLOW: 크롤러가 페이지 내의 링크를 따라가지 않도록 요청
    - INDEX: 크롤러가 페이지를 색인 가능 (기본 값)
    - FOLLOW: 크롤러가 페이지 내의 링크를 따라 감 (기본 값)
    - NOARCHIVE: 크롤러가 페이지를 캐시에 포함하지 않도록 요청. Google과 Bing 에서 사용
    - ALL: index, follow와 동일
    - NONE: noindex, nofollow와 동일

***검색엔진 META 태그***

- 그 외에 콘텐츠의 색인을 만드는데 사용되는 메타 태그들
    - DESCRIPTION: 페이지에 대한 짧고 정확한 요약.
        - `<meta name="description" content="여기는 골동품 상점">`
    - KEYWORDS: 페이지의 콘텐츠와 관련된, 쉼표로 구분한 키워드 목록.
        - `<meta name="keywords" content="antiques,mary,restoration">`

## 9.5 로봇 에티켓

- 로봇 차단 표준을 제안한 마틴 코스터(Martijn Koster) 로봇 제작자를 위한 가이드라인 목록 작성
    - 일부는 구식이지만 여전히 상당수는 유용하며, 현대적으로 고쳐쓴 가이드라인을 아래 소개

***(1) 신원 식별***

- 로봇의 신원을 밝히기
    - HTTP `User-Agent` 를 이용하여 로봇 이름 밝히기. 몇 로봇은 `User-Agent` 헤더에 목적과 정책을 기술한 URL을 포함하기도 함
- 기계의 신원 밝히기
    - 로봇이 DNS 엔트리를 가진 기계에서 실행되는 것을 확실히 해, 웹 사이트가 로봇의 IP 주소를 호스트 명을 통해 역방향 DNS 할 수 있도록 해야 함.
- 연락처를 밝히기
    - HTTP 폼 필드를 사용하여 연락할 수 있는 이메일 주소를 제공

***(2) 동작***

- 긴장하라
    - 로봇 운영자는 반드시 로봇이 바르게 행동하는지 감시해야 함
- 대비하라
    - 로봇을 실행하기 앞서 자신이 속한 조직에 알려둘 필요가 있음. 조직은 네트워크 대역폭의 소비를 감시하고, 어떤 문의에도 응할 준비를 하고 싶어 함.
- 감시와 로그
    - 로봇은 진행상황을 추적하고, 로봇 함정을 식별하고, 모든 것이 정상 동작하는지 기본적인 검사가 가능하도록 진단, 로깅 기능을 풍부하게 갖추어야 함.
- 배우고 조정하라
    - 크롤링을 할 때마다 새로운것을 배우게 될 것이며, 매번 로봇을 조정하고 개선하여 흔한 함정에 빠지는 것을 피해야 함

***(3) 스스로를 제한하라***

- URL을 필터링하라
    - 만약 URL을 이해할 수 없거나 관심 없는 데이터를 참조하는것 같다면 무시하는 것이 좋음
    - 예: URL의 확장자를 보고 판단. `.exe` 나 `.zip` 일 수 있음
- 동적 URL을 필터링하라
    - 보통 로봇들은 동적인 게이트웨이로부터 콘텐츠를 크롤링할 필요가 없음.
    - 게이트웨이에 적합한 쿼리를 보낼 줄 모를 것이며, 그 결과는 불규칙하거나 일시적임
- Accept 관련 헤더로 필터링
    - HTTP Accept 관련 헤더를 이용해 서버에게 어떤 콘텐츠를 이해할 수 있는지 말해줘야 함
- robots.txt 에 따르라
    - 방문한 웹 사이트의 robots.txt 제어를 따라야 함
- 스스로를 억제하라
    - 웹 사이트 접근 빈도를 세고, 이를 이용해 특정 사이트에 너무 자주 방문하지 않도록 해야 함.
    - 일반적으로 최대 분당 수 회 정도로 제한해야 하며, 각 요청 사이의 간격은 수 초 이상이 되어야 함. 총 접근 횟수도 제한되어야 함

***(4) 루프와 중복 견뎌내기, 그리고 그 외의 문제들***

- 모든 응답 코드 다루기
    - 리다이렉트, 에러를 포함한 모든 HTTP 상태 코드를 다룰 수 있어야 함. 또한 이에 대한 로그를 남기고 모니터링해야 함.
    - 특정 사이트의 응답이 성공이 아닌 것이 많다면, URL이 신선하지 않거나 서버가 로봇에 문서를 제공하지 않으려는 것일 수 있음
- URL 정규화하기
    - 모든 URL을 정규화해 같은 자원을 가리키는 중복 URL을 제거해야 함
- 적극적으로 순환 피하기
    - 순환을 감지하고 피하기 위해 많은 노력을 해야 함
- 함정 감시하라
    - 의도적이고 악의적인 순환을 잘 감시하라
- 블랙리스트를 관리하라
    - 함정, 사이클, 깨진 사이트, 로봇을 붙잡고싶은 사이트들을 블랙리스트에 추가해 방문하지 말아야 함

***(5) 확장성***

- 공간 이해하기
    - 풀고 있는 문제가 얼마나 큰지에 대해 미리 계산 필요. 로봇 작업 하나에 많은 메모리 요구.
- 대역폭 이해하기
    - 얼마나 네트워크 대역폭이 사용 가능한지, 요구 시간 내에 로봇작업이 끝내는데 얼마나 필요할지 이해할 것
    - 네트워크 사용량을 모니터링하면서 더 최적화하는 가능성을 찾을 수 있음
- 시간 이해하기
    - 로봇이 작업을 끝내는데 얼마나 많은 시간이 필요한지 이해하고, 추정시간과 실제 시간을 비교해야 함. 너무 다르다면 조사해봐야할만한 문제가 있을 수 있음.
- 분할 정복
    - 대규모 크롤링을 한다면 더 많은 하드웨어를 사용할 필요가 있음

***(6) 신뢰성***

- 철저하게 테스트하라
    - 로봇을 세상에 내놓기 전에 철저하게 테스트하고, 처음 밖에 내놓을 때는 작은 규모로 몇 차례 수행할 것
- 체크포인트
    - 어떤 로봇은 실패 위치에서 다시 시작하기 위해 진행상황의 스냅숏을 저장하고 싶어함
    - 대규모 로봇은 실패 할 때마다 처음부터 다시 시작할 수 없으므로 체크포인트/재시작 기능을 설계 해야 함
- 실패에 대한 유연성
    - 실패에 대비하여 로봇이 실패가 발생해도 계속 동작하도록 설계

***(7) 소통***

- 준비하라
    - 로봇에 대한 문의에 빠르게 응답할 수 있도록, 로봇 정책 안내 페이지와, 그 페이지에 robots.txt를 만드는 법을 포함해야 함
- 이해하라
    - 모든 문의자가 로봇에 대해 잘 아는 것이 아니므로 로봇 차단 규칙 표준에 대해 설명하고, 그래도 만족하지 못한다면 그 URL을 블랙리스트에 추가해야 함
- 즉각 대응하라
    - 로봇에 대한 항의/문의는 빠르게 대응해야 함

## 9.6 검색엔진

- 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색 엔진

### 9.6.1 넓게 생각하라

- 웹 초창기에는 검색엔진은 웹상 문서 위치를 알아내는 것을 돕는 상대적으로 단순한 데이터베이스
- 현재는 수많은 사용자와 수많은 웹 페이지에서 원하는 정보를 찾으므로 복잡한 크롤러를 사용해야 함

### 9.6.2 현대적인 검색엔진의 아키텍처

- 요즘 검색엔진들은 '풀 텍스트 색인(full-text indexes)'라는 복잡한 로컬 데이터베이스 생성
- 검색엔진 사용자는 웹 검색 게이트웨이를 통해 풀 텍스트 색인에 대해 질의 전송
    - 크롤링을 하는데는 시간이 걸리고 웹 페이지는 실시간으로 변하기때문에 풀 텍스트 색인은 기껏해야 특정 순간의 스냅숏에 불과

### 9.6.3 풀 텍스트 색인

- 풀 텍스트 색인: 입력 받은 단어를 포함하는 문서를 즉각 알려줄 수 있는 데이터베이스
    - 색인이 생성된 후에는 검색할 필요 없음

### 9.6.4 질의 보내기

- 사용자가 질의를 웹 검색엔진 게이트로 보내는 방법은 HTML 폼을 이용해 GET이나 POST 요청을 게이트웨이로 보냄
- 게이트웨이 프로그램은 검색 질의를 추출하고 풀텍스트 색인을 검색할 때 사용되는 형식으로 변환

### 9.6.5 검색 결과를 정렬하고 보여주기

- 질의의 결과를 확인하기 위해 검색엔진이 색인을 한번 사용 했다면, 게이트웨이 애플리케이션은 그 결과를 이용하여 최종 사용자를 위한 결과 페이지를 즉석으로 만들어 냄
- 많은 페이지가 검색어를 포함할 수 있으므로, 결과에 순위를 매기기위한 알고리즘을 사용
    - 관련도 랭킹(relevancy ranking): 검색 결과의 목록에 관련도로 점수를 매기고 정렬하는 과정
    - 이를 더 잘 지원하기 위해 많은 검색 엔진이 웹을 크롤링하는 과정에서 수집된 통계 데이터도 사용.

### 9.6.6 스푸핑

- 사용자들은 자신이 찾는 내용이 최상위 몇줄에서 보이지 않으면 불만족스러워 하므로, 검색 결과 순서는 중요
- 웹 마스터는 자신이 만든 사이트가 상단으로 만들 동기가 충분함
    - 이를 위해 관련없는 키워드를 나열하거나 가짜 페이지를 만들어 검색엔진을 더 잘 속일수 있도록하는 경우도 있음
- 검색엔진 및 로봇 구현자들은 이러한 속임수를 잘 잡아내기 위해 관련도 알고리즘을 계속 수정해야 함
