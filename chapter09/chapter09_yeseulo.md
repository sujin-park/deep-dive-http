# 9. 웹 로봇

* 웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션을 자동으로 수행하는 소프트웨어 프로그램이다.
* 많은 로봇이 웹 사이트에서 다른 웹 사이트로 떠돌아다니며 콘텐츠를 가져오고, 발견한 데이터를 처리한다.
* 이런 로봇은 자동으로 웹 사이트를 탐색하며 그 방식에 따라 '크롤러', '스파이더', '웜', '봇' 등 각양각색의 이름으로 불린다.
* 웹 로봇의 예
  * 주식시장 서버에 매 분 HTTP GET 요청을 보내고, 여기서 얻은 데이터를 활용해 주가 추이 그래프를 생성하는 주식 그래프 로봇
  * 월드 와이드 웹의 규모와 진화에 대한 통계 정보를 수집하는 웹 통계 조사 로봇
    * 웹을 떠돌며 페이지의 개수를 세고, 각 페이지의 크기, 언어, 미디어 타입을 기록한다. [netcraft.com](http://www.netcraft.com)
  * 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
  * 상품 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰 카탈로그에서 웹 페이지를 수집하는 가격 비교 로봇

## 9.1 크롤러와 크롤링

* 웹 크롤러는 재귀적으로 반복하는 방식으로 웹을 순회하는 로봇이다.
  * 웹 링크를 재귀적으로 따라가는 로봇을 크롤러 또는 스파이더라고 부른다. HTML 하이퍼링크로 만들어진 웹을 따라 '기어다니기(crawl)' 때문이다.
* 인터넷 검색엔진은 웹을 돌아다니며 만나는 모든 문서를 끌어오기 위해 크롤러를 사용한다.
  * 이 문서들은 나중에 처리되어 검색 가능한 데이터베이스로 만들어져, 사용자들이 특정 단어를 포함한 문서를 찾을 수 있게 한다.
  * 찾아서 가져와야 하는 페이지가 수십억 개나 있다 보니, 검색엔진 스파이더들은 가장 복잡한 로봇 중 하나다.

### 9.1.1 어디에서 시작하는가: '루트 집합'

* 크롤러가 동작하기 전 우선 출발지점을 주어야 한다. 크롤러가 방문을 시작하는 URL들의 초기 집합을 루트 집합(root set)이라고 한다.
* 루트 집합을 고를 때, 모든 링크를 크롤링하면 결과적으로 관심 있는 페이지 대부분을 가져올 수 있도록 충분히 다른 장소에서 URL들을 선택해야 한다.
  * 진짜 웹에서는 최종적으로 모든 문서로 이어지게 되는 하나의 문서란 없다.
  * 일반적으로 웹 대부분을 커버하기 위해 루트 집합에 너무 많은 페이지가 있을 필요는 없다.
* 보통 좋은 루트 집합은 크고 인기 있는 웹 사이트, 새로 생성된 페이지들의 목록, 자주 링크되지 않은 잘 알려지지 않은 페이지 목록으로 구성되어 있다.
* 인터넷 검색엔진에서 쓰이는 것과 같은 많은 대규모 크롤러 제품은 사용자들이 루트 집합에 새 페이지, 잘 알려지지 않은 페이지를 추가하는 기능을 제공한다.
* 이 루트 집합은 시간이 지남에 따라 성장하며 새 크롤링을 위한 시드 목록이 된다.

### 9.1.2 링크 추출과 상대 링크 정상화

* 크롤러는 검색한 각 페이지 안에 들어있는 URL 링크를 파싱하여 크롤링할 페이지 목록에 추가한다.
* 크롤링을 진행하며 탐색할 새 링크를 발견함에 따라 이 목록은 보통 급속히 확장된다.
* 크롤러는 간단한 HTML 파싱을 해서 링크를 추출하고 상대 링크를 절대 링크로 변환한다.

### 9.1.3 순환 피하기

* 로봇에 웹을 크롤링할 때 루프나 순환에 빠지지 않도록 매우 조심해야 한다.
* 로봇은 순환을 피하기 위해 반드시 어디를 방문했는지 알아야 한다. 순환은 로봇을 함정에 빠뜨려 멈추게 하거나 진행을 느려지게 한다.

### 9.1.4 루프와 중복

* 순환은 최소한 다음과 같은 이유로 크롤러에게 해롭다.
  * 순환은 크롤러를 루프에 빠뜨려 꼼짝 못하게 만들 수 있다. 루프는 허술하게 설계된 크롤러를 빙빙 돌게 할 수 있고, 같은 페이지를 반복해서 가져오는데 시간을 허비하게 만들 수 있다. 이런 크롤러가 네트워크 대역폭을 다 차지하여 다른 어떤 페이지도 가져올 수 없게 될 수 있다.
  * 크롤러가 같은 페이지를 반복해서 가져오면 고스란히 웹 서버의 부담이 된다. 크롤러의 네트워크 접근 속도가 충분히 빠르면, 웹 사이트를 압박하여 실제 사용자들이 사이트에 접근할 수 없게 막을 수도 있다. 이런 서비스 방해 행위는 법적 문제제기의 근거가 될 수 있다.
  * 루프 자체가 문제되지 않더라도, 크롤러는 수 많은 중복 페이지(dups)를 가져와서 쓸모없게 된다.

### 9.1.5 빵 부스러기의 흔적

* 방문한 곳을 지속적으로 추적하는 것은 쉽지 않다.
* 전 세계 웹 콘텐츠 상당 부분을 크롤링하려면 수십억 개의 URL을 방문해야 하고, 어떤 곳을 방문했는지 빠르게 판단하기 위해 복잡한 자료 구조를 사용할 필요가 있다.
  * 이 자료구조는 속도, 메모리 사용 면에서 효과적이어야 한다.
  * 로봇은 어떤 URL이 방문한 곳인지 빠른 결정을 위해 적어도 검색 트리나 해시 테이블을 필요로 할 것이다.
* 수억 개의 URL은 많은 공간을 차지한다. 평균 URL이 40바이트 길이이고, 웹 로봇이 5억 개의 URL을 크롤링 했다면, 검색 데이터 구조는 이 URL을 유지하기 위해 20GB 이상 메모리가 필요하다.

대규모 웹 크롤러가 방문한 곳 관리를 위해 사용하는 유용한 기법을 알아보자.

##### 트리와 해시 테이블
* 복잡한 로봇이 방문한 URL 추적을 위해 트리와 해시 테이블을 사용할 수 있다. 이는 URL을 훨씬 더 빨리 찾아볼 수 있게 하는 소프트웨어 자료 구조다.

##### 느슨한 존재 비트맵
* 공간 사용을 최소화하기 위해, 몇몇 대규모 크롤러는 존재 비트 배열(presence bit array)과 같은 느슨한 자료 구조를 사용한다.
* 각 URL은 해시 함수에 의해 고정된 크기의 숫자로 변환되고 배열 안에 대응하는 '존재 비트(presence bit)'를 갖는다.
* URL이 크롤링 되었을 때, 해당하는 존재 비트가 만들어진다. 존재 비트가 이미 존재한다면 그 URL은 이미 크롤링 되었다고 간주한다.

##### 체크포인트

* 로봇 프로그램이 갑작스레 중단될 경우를 대비해, 방문한 URL 목록이 디스크에 저장되었는지 확인한다.

##### 파티셔닝

* 웹이 성장하면서 한 대의 컴퓨터에서 하나의 로봇이 크롤링을 완수하는 것이 불가능해졌다.
  * 크롤링을 완수하기에 한 대의 컴퓨터로는 메모리, 디스크 공간, 연산 능력, 네트워크 대역폭이 충분치 않을 수 있다.
* 몇몇 대규모 웹 로봇은 각각이 분리된 한 대의 컴퓨터인 로봇들이 동시에 일하는 '농장(farm)'을 이용한다.
  * 각 로봇에 URL들의 특정 '한 부분'이 할당되어 그에 대한 책임을 지고 서로 도와 크롤링 한다.
  * 개별 로봇은 URL을 이리저리 넘겨주거나 오작동하는 동료를 돕거나, 그 외 이유로 활동을 조정하기 위한 커뮤니케이션을 할 수 있다.
* 거대 자료 구조 구현을 위한 참고 도서: [Managing Gigabytes: Compressing and indexing documents and images(1999)](https://www.amazon.com/Managing-Gigabytes-Compressing-Multimedia-Information/dp/1558605703)

### 9.1.6 별칭(alias)과 로봇 순환

* 올바른 자료 구조를 갖췄어도 URL이 별칭을 가질 수 있기 때문에 방문 여부를 체크하는게 쉽지 않을 수 있다.
* 한 URL이 또 다른 URL의 별칭이라면 이 둘이 서로 달라보여도 사실을 같은 리소스를 가리키고 있다.
* 같은 문서를 가리키는 다른 URL
  * 기본 포트 번호 80이 붙었을 때
  * `%7`이 `~`과 같을 때(이스케이핑 문자)
  * `#` 태그에 따라 페이지가 바뀌지 않을 때
  * 서버가 대소문자를 구분하지 않을 때
  * 기본 페이지가 index.html일 때
  * 호스트 명 대신 아이피 주소를 가질 때

### 9.1.7 URL 정규화하기

* 대부분 웹 로봇은 URL을 표준 형식으로 '정규화'하여 다른 URL과 같은 리소스를 가리키고 있는 확실한 것을 미리 제거하려고 한다.
  1. 포트 번호가 명시되지 않았다면, 호스트 명에 `:80`을 추가한다.
  2. 모든 `%xx` 이스케이핑 문자를 대응되는 문자로 변환한다.
  3. `#` 태그를 제거한다.
* 그 밖의 중복 문제 해결을 위해서는 웹 서버에 대한 지식이 필요하다.
  * 로봇은 웹 서버가 대소문자 구분을 하는지 알아야 한다.
  * 디렉터리에 대한 웹 서버의 색인 페이지 설정을 알아야 한다.
  * URL의 호스트 명과 IP 주소가 같은 물리적 컴퓨터를 참조한다는 것과, 웹 서버가 가상 호스팅을 하도록 설정되어 있는지 알아야 한다.
* URL 정규화는 기본적 문법의 별칭을 제거할 수 있으나, 로봇들은 URL을 표준 형식으로 변환하는 것 만으로 제거할 수 없는 다른 URL 별칭도 만나게 될 것이다.

### 9.1.8 파일 시스템 링크 순환

* 파일 시스템의 심벌릭 링크는 사실상 아무것도 존재하지 않으면서도 끝없이 깊어지는 디렉터리 계층을 만들 수 있기 때문에, 매우 교묘한 순환을 유발할 수 있다.
* 심벌릭 링크 순환은 보통 서버 관리자의 실수로 만들게 되지만, 때로 사악한 웹 마스터가 악의적으로 만들기도 한다.
  * 파일 시스템에서 `subdir/`이 상위 `/`를 가리키는 심벌릭 링크일 경우, `index.html`에서 이어지는 `subdir/index.html`을 가져와도 같은 `index.html`로 되돌아오며 순환된다.
  * URL이 달라 보이기 때문에 URL만으로는 문서가 같은 것을 모르고 루프에 빠질 수 있다.

### 9.1.9 동적 가상 웹 공간

* 악의적으로 복잡한 크롤러 루프를 만드는 경우도 있을 수 있다. 특히 평범한 파일처럼 보이지만 게이트웨이 애플리케이션인 URL을 만들기 쉽다.
* 이 애플리케이션은 같은 서버에 있는 가상 URL에 대한 링크를 포함한 HTML을 즉석에서 만들어낼 수 있다. 이 가상 URL로 요청을 받으면 새로운 가상 URL을 갖고 있는 새 HTML 페이지를 날조하여 만들어낸다.
* 악의적인 웹 서버는 실제로 파일을 하나도 갖고 있지 않으면서도 가상 웹 공간 너머로 로봇을 무한히 떠돌게 할 수 있다. URL과 HTML은 매번 전혀 달라보일 수 있기 때문에 로봇이 순환을 감지하기 매우 어렵다.
* 이보다 흔하게, 웹 마스터가 악의 없이 자신도 모르게 심벌릭 링크나 동적 콘텐츠를 통한 크롤러 함정을 만들 수 있다.
  * ex) 한 달 치 달력을 생성하고 그 다음 달로 링크를 걸어주는 CGI 기반 달력 프로그램: 콘텐츠의 동적 성질을 이해하지 못하는 로봇은 무한히 다음 달 달력을 요청할 수 있다.

### 9.1.10 루프와 중복 피하기

* 모든 순환을 피하는 완벽한 방법은 없다. 실제로 잘 설계된 로봇은 순환을 피하기 위해 휴리스틱의 집합을 필요로 한다.
* 이들 휴리스틱은 문제를 피하는데 도움을 주지만 동시에 약간 '손실'을 유발할 수도 있다. 의심스러워 보이지만 실은 유효한 콘텐츠를 거르게 될 수도 있기 때문이다.
* 웹에서 로봇이 문제를 일으킬 가능성이 크다. 이러한 웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법이 있다.

##### URL 정규화

* URL을 표준 형태로 변환함으로써 같은 리소스를 가리키는 중복 URL이 생기는 것을 일부 회피한다.

##### 너비 우선 크롤링

* 크롤러는 언제든 크롤링할 수 있는 URL의 큰 집합을 갖고 있다.
* 방문할 URL을 웹 사이트 전체에 걸쳐 너비 우선으로 스케쥴링하면, 순환의 영향을 최소화할 수 있다.
* 혹 로봇 함정을 건드리더라도, 여전히 그 순환에서 페이지를 받아오기 전에 다른 웹 사이트들에서 수십만 개의 페이지를 받아올 수 있다.
* 로봇을 깊이 우선 방식으로 운용하여 웹 사이트 하나에 성급히 뛰어들면, 순환을 건드리는 경우 영원이 다른 사이트로 빠져나올 수 없게 될 것이다.
* 너비 우선 크롤링은 요청을 더 분산시켜 특정 서버가 압박 받지 않도록 해주며, 한 로봇이 한 서버의 자원을 최소로 사용하도록 유지해준다.

##### 스로틀링

* 로봇이 웹 사이트에서 일정 시간동안 가져올 수 있는 페이지 숫자를 제한한다.
* 로봇이 순환을 건드려 지속적으로 그 사이트 별칭에 접근을 시도하면, 스로틀링을 이용해 그 서버에 대한 접근 횟수와 중복의 총 횟수를 제한할 수 있다.

##### URL 크기 제한

* 로봇은 일정 길이(보통 1KB)를 넘는 URL의 크롤링은 거부할 수 있다. 순환으로 인해 URL이 계속해서 길어진다면 길이 제한으로 순환이 중단될 수 있다.
* 어떤 웹 서버들은 긴 URL이 주어진 경우 실패하고 URL이 점점 길어지는 순환에 빠진 로봇은 이런 웹 서버와 충돌을 유발할 수 있다. 이는 웹 마스터가 로봇을 서비스 거부 공격자로 오해하게 만든다.
* 이 기법으로 가져오지 못하는 콘텐츠가 있을 수 있음을 주의해야 한다.
  * 오늘날 많은 사이트가 URL을 사용자 상태 관리에 사용한다. (사용자 아이디를 페이지에서 참조하는 URL에 저장한다)
* URL 길이는 크롤링을 제한하는 방법으로는 까다로울 수 있으나 요청 URL이 특정 크기에 도달할 때마다 에러 로그를 남김으로써, 특정 사이트에서 어떤 일이 벌어지는지 감시하는 사용자에게 유용한 신호를 제공한다.

##### URL/사이트 블랙리스트

* 로봇 순환을 만들거나 함정으로 알려진 사이트와 URL 목록을 만들어 관리하고 피한다.
* 문제를 일으키는 사이트, URL을 발견할 때마다 이 블랙리스트에 추가한다.
* 이는 사람의 손을 필요로 한다. 그러나 오늘날의 대부분 대규모 크롤러는 블랙리스트를 가지고 있다.
* 블랙리스트는 크롤링되는 것을 싫어하는 특정 사이트를 피하기 위해 사용될 수 있다.

##### 패턴 발견

* 파일 시스템의 심벌릭 링크를 통한 순환과 그와 비슷한 오설정은 일정 패턴을 따르는 경향이 있다.
* 예를 들어 URL은 중복된 구성 요소와 함께 점점 길어질 수 있는데, 몇몇 로봇은 반복되는 구성 요소를 가진 URL을 잠재적 순환으로 보고 크롤링을 거절한다.
* 반복이 단순하지 않고 주기의 길이가 2 이상인 것도 있다. 몇몇 로봇은 다른 주기의 반복 패턴을 감지해낸다.

##### 콘텐츠 지문(fingerprint)

* 지문은 더 복잡한 웹 크롤러들이 중복을 감지하는 보다 직접적인 방법이다.
* 콘텐츠 지문을 사용하는 로봇들은 페이지 콘텐츠에서 몇 바이트를 얻어내 체크섬(checksum)을 계산한다. 이 체크섬은 그 페이지 내용의 간략한 표현이다.
* 로봇이 이전에 봤던 체크섬을 가진 페이지를 가져오면, 그 페이지의 링크는 크롤링하지 않는다. 이미 크롤링이 시작된 상태이기 때문이다.
* 체크섬 함수는 어떤 두 페이지가 서로 내용이 다름에도 체크섬은 똑같은 확률이 적은 것을 사용해야 한다. 지문 생성용으로 MD5와 같은 메시지 요약 함수가 인기 있다.
* 어떤 웹 서버는 동적으로 그때그때 페이지를 수정하므로, 로봇은 때로 웹 페이지 콘텐츠에 임베딩된 링크와 같은 특정 부분을 체크섬 계산에서 빠뜨린다.
* 뿐만 아니라 페이지 콘텐츠를 임의로 커스터마이징하는 것(날짜 추가, 카운터 접근 등)을 포함한 서버 측 동적인 동작은 중복 감지를 방해할 수 있다.

##### 사람의 모니터링

* 로봇이 어떤 기법으로도 해결할 수 없는 문제에 봉착하게 될 것이다.
* 사람이 쉽게 로봇 진행 상황을 모니터링해서 문제가 일어나면 즉각 인지할 수 있게끔 반드시 진단과 로깅을 포함하여 로봇을 설계해야 한다.


* 웹 처럼 거대한 데이터 집합을 크롤링하기 위한 좋은 스파이더 휴리스틱을 만드는 작업은 언제나 현재진행형이며, 진화한다.
* 더 작고 더 커스터마이징된 크롤러는 그들이 어떤 자원에 얼마나 영향을 줄 것인지 스스로 제어할 수 있거나, 혹은 그 자원 자체가 크롤링을 수행하는 사람의 제어하에 있을 수 있기 때문에 이 문제 일부를 피해갈 수 있다. 이 크롤러는 문제 예방을 위해 사람의 모니터링에 더욱 의존한다.

## 9.2 로봇의 HTTP

* 로봇도 다른 HTTP 클라이언트 프로그램처럼 HTTP 명세 규칙을 지켜야 한다.
* HTTP 요청을 만들고 스스로를 HTTP/1.1 클라이언트라고 광고하는 로봇은 적절한 HTTP 요청 헤더를 사용해야 한다.
* 많은 로봇이 콘텐츠 요청을 위해 필요한 HTTP를 최소한으로만 구현하려 한다. 이는 문제를 유발할 수 있지만 이런 행태가 빨리 바뀔 것 같지는 않다.
  * 결과적으로 많은 로봇이 요구사항이 적은 HTTP/1.0 요청을 보낸다.

### 9.2.1 요청 헤더 식별하기

* 로봇들이 HTTP를 최소한도로만 지원하려 함에도 불구하고, 대부분 약간의 신원 식별 헤더(특히 User-Agent HTTP 헤더)를 구현하고 전송한다.
* 로봇 구현자들은 로봇의 능력, 신원, 출신을 알려주는 기본적인 몇 헤더를 사이트에게 보내주는 것이 좋다.
* 이는 잘못된 크롤러의 소유자를 찾아낼 때와 서버에게 로봇이 어떤 종류의 콘텐츠를 다룰 수 있는지에 대한 약간의 정보를 주려 할 때 유용하다.
  * User-Agent: 서버에게 요청을 만든 로봇의 이름을 말해준다.
  * From: 로봇의 사용자/관리자의 이메일 주소를 제공한다.
  * Accept: 서버에게 어떤 미디어 타입을 보내도 되는지 말해준다. 로봇이 관심있는 유형의 콘텐츠만 받게 될 것임을 확신하는데 도움을 준다.
  * Referer: 현재 요청 URL을 포함한 문서의 URL을 제공한다.

### 9.2.2 가상 호스팅

* 로봇 구현자들은 Host 헤더를 지원할 필요가 있다. 가상 호스팅이 널리 퍼진 현실에서 요청에 Host 헤더를 포함하지 않으면 로봇이 어떤 URL에 대해 잘못된 콘텐츠를 찾게 만든다. 이런 이유로 HTTP/1.1은 Host 헤더를 사용할 것을 요구한다.
* 대부분 서버는 기본적으로 특정 사이트 하나를 운영하도록 설정되어 있다. 따라서 Host 헤더를 포함하지 않은 크롤러는 a, b 두 개의 사이트를 운영하는 서버에 b에 대한 요청을 보낼 수 있다.
  * 서버가 a를 기본으로 사용하도록 설정되어 있다면(그리고 Host 헤더를 요구하지 않는다면) 크롤러가 a 사이트 콘텐츠를 얻게 하고, 더 나쁘게는 a 콘텐츠를 b 사이트에서 온 것이라고 간주할 수 있다.

### 9.2.3 조건부 요청

* 수십억 개의 웹 페이지를 다운받게 될 수도 있는 인터넷 검색 엔진 로봇과 같은 경우, 오직 변경되었을 때만 콘텐츠를 가져오도록 하는 것은 의미 있다.
* 이 로봇 중 몇몇은 시간이나 엔터티 태그를 비교하여 그들이 받아간 마지막 버전 이후 업데이트된 것이 있는지 알아보는 조건부 HTTP 요청을 구현한다.
* 이는 HTTP 캐시가 전에 받아온 리소스의 로컬 사본이 유효성을 검사하는 방법과 매우 비슷하다.

### 9.2.4 응답 다루기

* 대다수 로봇의 주 관심사는 단순히 GET 메서드로 콘텐츠를 요청해서 가져오는 것이기 때문에 응답 다루기라 부를 만한 일은 거의 하지 않는다.
* 그러나 HTTP의 몇몇 기능(조건부 요청과 같은)을 사용하는 로봇들이나, 웹 탐색이나 서버와의 상호작용을 더 잘해보려 하는 로봇은 여러 종류의 HTTP 응답을 다룰 수 있어야 한다.

##### 상태 코드

* 최소한의 일반적이고 예상할 수 있는 상태 코드를 다룰 수 있어야 한다.
* 모든 로봇은 `200 OK`나 `404 Not Found` 같은 HTTP 상태 코드를 이해해야 한다. 명시적으로 이해할 수 없는 상태 코드는 상태 코드가 속한 분류에 근거해 다뤄야 한다.
* 모든 서버가 언제나 항상 적절한 에러 코드를 반환하지 않는다는 것을 알아야 한다. 몇 서버는 에러를 기술하는 메시지도 `200 OK`로 응답하기도 한다.

##### 엔터티

* HTTP 헤더에 임베딩된 정보를 따라 로봇은 엔터티 자체의 정보를 찾을 수 있다.
* 메타 http-equiv 태그와 같은 메타 HTML 태그는 리소스에 대해 콘텐츠 저자가 포함시킨 정보다.
  * 콘텐츠 저자는 http-equiv 태그를 이용해 콘텐츠를 다루는 서버가 제공할 수 있는 헤더를 덮어쓰기할 수 있다.
    ```
    <meta http-equiv="Refresh" content="1; URL=index.html">
    ```
  * 이 태그는 수신자가 그 문서의 HTTP 응답 값이 `1; URL=index.html`인 Refresh HTTP 헤더를 포함한 것처럼 다루게 한다.
* 몇 서버는 HTML 페이지를 보내기 전 그 내용을 파싱하여 http-equiv 태그를 헤더로 포함시키지만 어떤 서버는 그렇게 하지 않는다.
* 로봇 구현자들은 http-equiv 정보를 찾아내기 위해 HTML 문서의 HEAD 태그를 탐색해야 할 수 있다.

### 9.2.5 User-Agent 타기팅

* 웹 관리자는 로봇들의 방문과 요청을 예상해야 한다.
* 많은 웹 사이트는 그들의 여러 기능을 지원하도록 브라우저 종류를 감지해 그에 맞게 콘텐츠를 최적화한다. 이로써 사이트는 로봇에게 콘텐츠 대신 에러 페이지를 제공한다.
  * "당신의 브라우저는 프레임을 지원하지 않습니다.(your browser does not support frames)"
* 로봇의 요청을 다루기 위해, 특정 브라우저의 기능이 지원되는 것을 전제로 콘텐츠를 개발하는 대신, 풍부한 기능을 갖추지 못한 브라우저나 로봇 등 다양한 클라이언트에 대응하는 유연한 웹 페이지를 개발할 수 있다.
* 최소한 로봇이 사이트에 방문했다가 콘텐츠를 얻지 못하는 일이 없도록 대비해야 한다.

## 9.3 부적절하게 동작하는 로봇들

로봇이 저지르는 실수와 그로 인해 초래되는 결과

##### 폭주하는 로봇

* 로봇은 웹 서핑을 하는 사람보다 훨씬 빠르게 HTTP 요청을 만들 수 있으며, 빠른 네트워크 연결을 갖춘 빠른 컴퓨터 위에 동작할 수 있다.
* 로봇이 논리적 에러를 갖고 있거나 순환에 빠졌다면 웹 서버에 과부하를 유발할 수 있으며, 다른 누구에게도 서비스를 못하게 만들 수 있다.
* 로봇의 폭주 방지를 위한 보호 장치를 반드시 신경 써서 설계해야 한다.

##### 오래된 URL

* 로봇이 방문하는 URL의 목록이 오래되었을 수 있다. 웹 사이트가 콘텐츠를 많이 바꾸었다면, 로봇들은 존재하지 않는 URL에 대한 요청을 많이 보낼 수 있다.
* 존재하지 않는 문서에 대한 접근 요청으로 에러 로그가 채워지거나, 에러 페이지를 제공하는 부하로 인해 웹 서버의 요청에 대한 수용 능력이 감소될 수 있다.

##### 길고 잘못된 URL

* 순환이나 프로그래밍상 오류로 로봇은 웹사이트에게 크고 의미 없는 URL을 요청할 수 있다.
* URL이 너무 크면 웹 서버의 처리 능력에 영향을 주고, 웹 서버의 접근 로그를 어지럽게 채울 수 있으며, 허술한 웹 서버라면 고장을 일으킬 수 있다.

##### 호기심이 지나친 로봇

* 어떤 로봇은 사적인 데이터의 URL을 얻어 검색엔진이나 기타 애플리케이션을 통해 쉽게 접근할 수 있게 만들어 사생활 침해를 일으킬 수 있다.
* 보통 이는 사적인 콘텐츠에 대해 이미 존재하는 하이퍼링크를 로봇이 따라감으로써 벌어지는 일이다: 소유자가 비밀이라고 생각했으나 실은 그렇지 않은 콘텐츠, 소유자가 하이퍼링크 제거를 깜박한 경우
* 이는 로봇이 명시적으로 하이퍼링크가 존재하지도 않은 문서들을 디렉터리의 콘텐츠를 가져오는 등의 방법으로 긁어올 때 일어날 수 있다.
* 민감한 데이터를 무시하는, 검색 색인이나 아카이브에서 제거하는 메커니즘은 중요하다.
* 구글과 같은 몇몇 검색엔진은 그들이 크롤링한 페이지를 그대로 보관하므로 콘텐츠가 제거되더라도 일정 시간동안 여전히 검색되고 접근 가능하다.

##### 동적 게이트웨이 접근

* 로봇이 그들이 접근하는 것에 대해 언제나 잘 아는 것은 아니다.
* 로봇은 게이트웨이 애플리케이션의 콘텐츠에 대한 URL로 요청을 할 수도 있다.
* 이 경우 얻은 데이터는 아마 특수 목적을 위한 것일 테고 처리 비용이 많이 들 수 있다.
* 많은 웹사이트 관리자가 게이트웨이에서 얻은 문서를 요청하는 순진한 로봇을 좋아하지 않는다.